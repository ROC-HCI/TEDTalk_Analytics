(dp1
S'talk_transcript'
p2
(lp3
(lp4
VI work with a bunch of mathematicians, philosophers and computer scientists,
p5
aVand we sit around and think about the future of machine intelligence,
p6
aVamong other things.
p7
aVSome people think that some of these things are sort of science fiction-y,
p8
aVfar out there, crazy.
p9
aVBut I like to say,
p10
aVokay, let's look at the modern human condition.
p11
aV(Laughter)
p12
aVThis is the normal way for things to be.
p13
aa(lp14
VBut if we think about it,
p15
aVwe are actually recently arrived guests on this planet,
p16
aVthe human species.
p17
aVThink about if Earth was created one year ago,
p18
aVthe human species, then,  would be 10 minutes old.
p19
aVThe industrial era started two seconds ago.
p20
aVAnother way to look at this is to think of world GDP over the last 10,000 years,
p21
aVI've actually taken the trouble to plot this for you in a graph.
p22
aVIt looks like this.
p23
aV(Laughter)
p24
aVIt's a curious shape for a normal condition.
p25
aVI sure wouldn't want to sit on it.
p26
aV(Laughter)
p27
aa(lp28
VLet's ask ourselves, what is the cause of this current anomaly?
p29
aVSome people would say it's technology.
p30
aVNow it's true, technology has accumulated through human history,
p31
aVand right now, technology advances extremely rapidly \u2014
p32
aVthat is the proximate cause,
p33
aVthat's why we are currently  so very productive.
p34
aVBut I like to think back further  to the ultimate cause.
p35
aa(lp36
VLook at these two highly distinguished gentlemen:
p37
aVWe have Kanzi \u2014
p38
aVhe's mastered 200 lexical tokens, an incredible feat.
p39
aVAnd Ed Witten unleashed the second superstring revolution.
p40
aVIf we look under the hood,  this is what we find:
p41
aVbasically the same thing.
p42
aVOne is a little larger,
p43
aVit maybe also has a few tricks in the exact way it's wired.
p44
aVThese invisible differences cannot be too complicated, however,
p45
aVbecause there have only been 250,000 generations
p46
aVsince our last common ancestor.
p47
aVWe know that complicated mechanisms take a long time to evolve.
p48
aVSo a bunch of relatively minor changes
p49
aVtake us from Kanzi to Witten,
p50
aVfrom broken-off tree branches to intercontinental ballistic missiles.
p51
aa(lp52
VSo this then seems pretty obvious that everything we've achieved,
p53
aVand everything we care about,
p54
aVdepends crucially on some relatively minor changes that made the human mind.
p55
aVAnd the corollary, of course, is that any further changes
p56
aVthat could significantly change the substrate of thinking
p57
aVcould have potentially  enormous consequences.
p58
aa(lp59
VSome of my colleagues  think we're on the verge
p60
aVof something that could cause a profound change in that substrate,
p61
aVand that is machine superintelligence.
p62
aVArtificial intelligence used to be about putting commands in a box.
p63
aVYou would have human programmers
p64
aVthat would painstakingly  handcraft knowledge items.
p65
aVYou build up these expert systems,
p66
aVand they were kind of useful  for some purposes,
p67
aVbut they were very brittle, you couldn't scale them.
p68
aVBasically, you got out only what you put in.
p69
aVBut since then,
p70
aVa paradigm shift has taken place in the field of artificial intelligence.
p71
aa(lp72
VToday, the action is really  around machine learning.
p73
aVSo rather than handcrafting knowledge representations and features,
p74
aVwe create algorithms that learn, often from raw perceptual data.
p75
aVBasically the same thing that the human infant does.
p76
aVThe result is A.I. that is not limited to one domain \u2014
p77
aVthe same system can learn to translate  between any pairs of languages,
p78
aVor learn to play any computer game on the Atari console.
p79
aVNow of course,
p80
aVA.I. is still nowhere near having the same powerful, cross-domain
p81
aVability to learn and plan as a human being has.
p82
aVThe cortex still has some  algorithmic tricks
p83
aVthat we don't yet know how to match in machines.
p84
aa(lp85
VSo the question is,
p86
aVhow far are we from being able to match those tricks?
p87
aVA couple of years ago,
p88
aVwe did a survey of some of the world's  leading A.I. experts,
p89
aVto see what they think, and one of the questions we asked was,
p90
aV"By which year do you think there is a 50 percent probability
p91
aVthat we will have achieved  human-level machine intelligence?"
p92
aVWe defined human-level here  as the ability to perform
p93
aValmost any job at least as well as an adult human,
p94
aVso real human-level, not just within some limited domain.
p95
aVAnd the median answer was 2040 or 2050,
p96
aVdepending on precisely which  group of experts we asked.
p97
aVNow, it could happen much, much later, or sooner,
p98
aVthe truth is nobody really knows.
p99
aa(lp100
VWhat we do know is that the ultimate  limit to information processing
p101
aVin a machine substrate lies far outside  the limits in biological tissue.
p102
aVThis comes down to physics.
p103
aVA biological neuron fires, maybe,  at 200 hertz, 200 times a second.
p104
aVBut even a present-day transistor operates at the Gigahertz.
p105
aVNeurons propagate slowly in axons, 100 meters per second, tops.
p106
aVBut in computers, signals can travel at the speed of light.
p107
aVThere are also size limitations,
p108
aVlike a human brain has  to fit inside a cranium,
p109
aVbut a computer can be the size of a warehouse or larger.
p110
aVSo the potential for superintelligence  lies dormant in matter,
p111
aVmuch like the power of the atom  lay dormant throughout human history,
p112
aVpatiently waiting there until 1945.
p113
aVIn this century,
p114
aVscientists may learn to awaken the power of artificial intelligence.
p115
aVAnd I think we might then see an intelligence explosion.
p116
aa(lp117
VNow most people, when they think about what is smart and what is dumb,
p118
aVI think have in mind a picture roughly like this.
p119
aVSo at one end we have the village idiot,
p120
aVand then far over at the other side
p121
aVwe have Ed Witten, or Albert Einstein, or whoever your favorite guru is.
p122
aVBut I think that from the point of view of artificial intelligence,
p123
aVthe true picture is actually probably more like this:
p124
aVAI starts out at this point here, at zero intelligence,
p125
aVand then, after many, many  years of really hard work,
p126
aVmaybe eventually we get to mouse-level artificial intelligence,
p127
aVsomething that can navigate  cluttered environments
p128
aVas well as a mouse can.
p129
aVAnd then, after many, many more years of really hard work, lots of investment,
p130
aVmaybe eventually we get to chimpanzee-level artificial intelligence.
p131
aVAnd then, after even more years  of really, really hard work,
p132
aVwe get to village idiot  artificial intelligence.
p133
aVAnd a few moments later,  we are beyond Ed Witten.
p134
aVThe train doesn't stop at Humanville Station.
p135
aVIt's likely, rather, to swoosh right by.
p136
aa(lp137
VNow this has profound implications,
p138
aVparticularly when it comes  to questions of power.
p139
aVFor example, chimpanzees are strong \u2014
p140
aVpound for pound, a chimpanzee is about twice as strong as a fit human male.
p141
aVAnd yet, the fate of Kanzi  and his pals depends a lot more
p142
aVon what we humans do than on what the chimpanzees do themselves.
p143
aVOnce there is superintelligence,
p144
aVthe fate of humanity may depend on what the superintelligence does.
p145
aVThink about it:
p146
aVMachine intelligence is the last invention that humanity will ever need to make.
p147
aVMachines will then be better  at inventing than we are,
p148
aVand they'll be doing so  on digital timescales.
p149
aVWhat this means is basically a telescoping of the future.
p150
aVThink of all the crazy technologies  that you could have imagined
p151
aVmaybe humans could have developed in the fullness of time:
p152
aVcures for aging, space colonization,
p153
aVself-replicating nanobots or uploading of minds into computers,
p154
aVall kinds of science fiction-y stuff
p155
aVthat's nevertheless consistent  with the laws of physics.
p156
aVAll of this superintelligence could  develop, and possibly quite rapidly.
p157
aa(lp158
VNow, a superintelligence with such  technological maturity
p159
aVwould be extremely powerful,
p160
aVand at least in some scenarios, it would be able to get what it wants.
p161
aVWe would then have a future that would be shaped by the preferences of this A.I.
p162
aVNow a good question is, what are those preferences?
p163
aVHere it gets trickier.
p164
aVTo make any headway with this,
p165
aVwe must first of all avoid anthropomorphizing.
p166
aVAnd this is ironic because  every newspaper article
p167
aVabout the future of A.I. has a picture of this:
p168
aVSo I think what we need to do is to conceive of the issue more abstractly,
p169
aVnot in terms of vivid Hollywood scenarios.
p170
aa(lp171
VWe need to think of intelligence  as an optimization process,
p172
aVa process that steers the future into a particular set of configurations.
p173
aVA superintelligence is a really strong optimization process.
p174
aVIt's extremely good at using  available means to achieve a state
p175
aVin which its goal is realized.
p176
aVThis means that there is no necessary connection between
p177
aVbeing highly intelligent in this sense,
p178
aVand having an objective that we humans would find worthwhile or meaningful.
p179
aa(lp180
VSuppose we give an A.I. the goal  to make humans smile.
p181
aVWhen the A.I. is weak, it performs useful or amusing actions
p182
aVthat cause its user to smile.
p183
aVWhen the A.I. becomes superintelligent,
p184
aVit realizes that there is a more effective way to achieve this goal:
p185
aVtake control of the world
p186
aVand stick electrodes into the facial muscles of humans
p187
aVto cause constant, beaming grins.
p188
aVAnother example,
p189
aVsuppose we give A.I. the goal to solve a difficult mathematical problem.
p190
aVWhen the A.I. becomes superintelligent,
p191
aVit realizes that the most effective way  to get the solution to this problem
p192
aVis by transforming the planet into a giant computer,
p193
aVso as to increase its thinking capacity.
p194
aVAnd notice that this gives the A.I.s an instrumental reason
p195
aVto do things to us that we might not approve of.
p196
aVHuman beings in this model are threats,
p197
aVwe could prevent the mathematical problem from being solved.
p198
aa(lp199
VOf course, perceivably things won't  go wrong in these particular ways;
p200
aVthese are cartoon examples.
p201
aVBut the general point here is important:
p202
aVif you create a really powerful optimization process
p203
aVto maximize for objective x,
p204
aVyou better make sure  that your definition of x
p205
aVincorporates everything you care about.
p206
aVThis is a lesson that's also taught in many a myth.
p207
aVKing Midas wishes that everything he touches be turned into gold.
p208
aVHe touches his daughter,  she turns into gold.
p209
aVHe touches his food, it turns into gold.
p210
aVThis could become practically relevant,
p211
aVnot just as a metaphor for greed,
p212
aVbut as an illustration of what happens
p213
aVif you create a powerful optimization process
p214
aVand give it misconceived  or poorly specified goals.
p215
aa(lp216
VNow you might say, if a computer starts sticking electrodes into people's faces,
p217
aVwe'd just shut it off.
p218
aVA, this is not necessarily so easy to do if we've grown dependent on the system \u2014
p219
aVlike, where is the off switch  to the Internet?
p220
aVB, why haven't the chimpanzees flicked the off switch to humanity,
p221
aVor the Neanderthals?
p222
aVThey certainly had reasons.
p223
aVWe have an off switch,  for example, right here.
p224
aV(Choking)
p225
aVThe reason is that we are  an intelligent adversary;
p226
aVwe can anticipate threats  and plan around them.
p227
aVBut so could a superintelligent agent,
p228
aVand it would be much better  at that than we are.
p229
aVThe point is, we should not be confident that we have this under control here.
p230
aa(lp231
VAnd we could try to make our job a little bit easier by, say,
p232
aVputting the A.I. in a box,
p233
aVlike a secure software environment,
p234
aVa virtual reality simulation from which it cannot escape.
p235
aVBut how confident can we be that the A.I. couldn't find a bug.
p236
aVGiven that merely human hackers find bugs all the time,
p237
aVI'd say, probably not very confident.
p238
aVSo we disconnect the ethernet cable to create an air gap,
p239
aVbut again, like merely human hackers
p240
aVroutinely transgress air gaps using social engineering.
p241
aVRight now, as I speak,
p242
aVI'm sure there is some employee out there somewhere
p243
aVwho has been talked into handing out  her account details
p244
aVby somebody claiming to be from the I.T. department.
p245
aa(lp246
VMore creative scenarios are also possible,
p247
aVlike if you're the A.I.,
p248
aVyou can imagine wiggling electrodes around in your internal circuitry
p249
aVto create radio waves that you can use to communicate.
p250
aVOr maybe you could pretend to malfunction,
p251
aVand then when the programmers open you up to see what went wrong with you,
p252
aVthey look at the source code \u2014 Bam! \u2014
p253
aVthe manipulation can take place.
p254
aVOr it could output the blueprint to a really nifty technology,
p255
aVand when we implement it,
p256
aVit has some surreptitious side effect that the A.I. had planned.
p257
aVThe point here is that we should  not be confident in our ability
p258
aVto keep a superintelligent genie locked up in its bottle forever.
p259
aVSooner or later, it will out.
p260
aa(lp261
VI believe that the answer here is to figure out
p262
aVhow to create superintelligent A.I. such that even if \u2014 when \u2014 it escapes,
p263
aVit is still safe because it is fundamentally on our side
p264
aVbecause it shares our values.
p265
aVI see no way around  this difficult problem.
p266
aa(lp267
VNow, I'm actually fairly optimistic that this problem can be solved.
p268
aVWe wouldn't have to write down  a long list of everything we care about,
p269
aVor worse yet, spell it out  in some computer language
p270
aVlike C++ or Python,
p271
aVthat would be a task beyond hopeless.
p272
aVInstead, we would create an A.I. that uses its intelligence
p273
aVto learn what we value,
p274
aVand its motivation system is constructed in such a way that it is motivated
p275
aVto pursue our values or to perform actions that it predicts we would approve of.
p276
aVWe would thus leverage  its intelligence as much as possible
p277
aVto solve the problem of value-loading.
p278
aa(lp279
VThis can happen,
p280
aVand the outcome could be  very good for humanity.
p281
aVBut it doesn't happen automatically.
p282
aVThe initial conditions  for the intelligence explosion
p283
aVmight need to be set up  in just the right way
p284
aVif we are to have a controlled detonation.
p285
aVThe values that the A.I. has need to match ours,
p286
aVnot just in the familiar context,
p287
aVlike where we can easily check how the A.I. behaves,
p288
aVbut also in all novel contexts that the A.I. might encounter
p289
aVin the indefinite future.
p290
aa(lp291
VAnd there are also some esoteric issues that would need to be solved, sorted out:
p292
aVthe exact details of its decision theory,
p293
aVhow to deal with logical uncertainty and so forth.
p294
aVSo the technical problems that need to be solved to make this work
p295
aVlook quite difficult \u2014
p296
aVnot as difficult as making  a superintelligent A.I.,
p297
aVbut fairly difficult.
p298
aVHere is the worry:
p299
aVMaking superintelligent A.I. is a really hard challenge.
p300
aVMaking superintelligent A.I. that is safe
p301
aVinvolves some additional  challenge on top of that.
p302
aVThe risk is that if somebody figures out how to crack the first challenge
p303
aVwithout also having cracked  the additional challenge
p304
aVof ensuring perfect safety.
p305
aa(lp306
VSo I think that we should work out a solution
p307
aVto the control problem in advance,
p308
aVso that we have it available  by the time it is needed.
p309
aVNow it might be that we cannot solve the entire control problem in advance
p310
aVbecause maybe some elements can only be put in place
p311
aVonce you know the details of the  architecture where it will be implemented.
p312
aVBut the more of the control problem that we solve in advance,
p313
aVthe better the odds that the transition to the machine intelligence era
p314
aVwill go well.
p315
aa(lp316
VThis to me looks like a thing that is well worth doing
p317
aVand I can imagine that if  things turn out okay,
p318
aVthat people a million years from now look back at this century
p319
aVand it might well be that they say that the one thing we did that really mattered
p320
aVwas to get this thing right.
p321
aa(lp322
VThank you.
p323
aa(lp324
V(Applause)
p325
aasS'transcript_micsec'
p326
(lp327
I11000
aI40000
aI78000
aI104000
aI151000
aI175000
aI209000
aI258000
aI304000
aI369000
aI433000
aI503000
aI548000
aI578000
aI628000
aI675000
aI723000
aI765000
aI806000
aI823000
aI863000
aI893000
aI936000
aI965000
aI983000
aI985000
asS'talk_meta'
p328
(dp329
S'ratings'
p330
(dp331
S'ingenious'
p332
I116
sS'funny'
p333
I25
sS'inspiring'
p334
I316
sS'ok'
p335
I130
sS'fascinating'
p336
I373
sS'total_count'
p337
I2335
sS'persuasive'
p338
I218
sS'longwinded'
p339
I63
sS'informative'
p340
I684
sS'beautiful'
p341
I27
sS'jaw-dropping'
p342
I97
sS'obnoxious'
p343
I13
sS'confusing'
p344
I45
sS'courageous'
p345
I148
sS'unconvincing'
p346
I80
ssS'author'
p347
VNick_Bostrom;
p348
sS'url'
p349
S'https://www.ted.com/talks/nick_bostrom_what_happens_when_our_computers_get_smarter_than_we_are'
p350
sS'vidlen'
p351
I991
sS'totalviews'
p352
I2897720
sS'title'
p353
VWhat happens when our computers get smarter than we are?
p354
sS'downloadlink'
p355
Vhttps://download.ted.com/talks/NickBostrom_2015.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22
p356
sS'datepublished'
p357
cdatetime
datetime
p358
(S'\x07\xdf\x04\x1b\x0b\x00\x11\x00\x00\x00'
tRp359
sS'datefilmed'
p360
g358
(S'\x07\xdf\x04\x1b\x0b\x00\x11\x00\x00\x00'
tRp361
sS'alldata_JSON'
p362
S'{"viewed_count": 2897720, "speakers": [{"description": "Philosopher", "firstname": "Nick", "title": "", "lastname": "Bostrom", "middleinitial": "", "whylisten": "<p>Philosopher Nick Bostrom envisioned a future full of human enhancement, nanotechnology and machine intelligence long before they became mainstream concerns. From his famous simulation argument -- which identified some striking implications of rejecting the Matrix-like idea that humans are living in a computer simulation -- to his work on existential risk, Bostrom approaches both the inevitable and the speculative using the tools of philosophy, probability theory, and scientific analysis.<br /><br />Since 2005, Bostrom has led the <a href=\\"http://www.fhi.ox.ac.uk/\\" target=\\"_blank\\">Future of Humanity Institute</a>, a research group of mathematicians, philosophers and scientists at Oxford University tasked with investigating the big picture for the human condition and its future. He has been referred to as one of the most important thinkers of our age.</p><p>Nick was honored as one of <em>Foreign Policy</em>&#39;s 2015&nbsp;<a href=\\"http://2015globalthinkers.foreignpolicy.com/?utm_source=Sailthru&amp;utm_medium=email&amp;utm_campaign=New%20Campaign&amp;utm_term=Flashpoints#!advocates/detail/bostrom\\" target=\\"_blank\\">Global Thinkers</a> .</p><p>His recent book <a href=\\"http://geni.us/superintelligence\\" target=\\"_blank\\"><em>Superintelligence</em></a>  advances the ominous idea that &ldquo;the first ultraintelligent machine is the last invention that man need ever make.&rdquo;</p>", "slug": "nick_bostrom", "whotheyare": "Nick Bostrom asks big questions: What should we do, as individuals and as a species, to optimize our long-term prospects? Will humanity\\u2019s technological advancements ultimately destroy us?", "whatotherssay": "Bostrom cogently argues that the prospect of superintelligent machines is \\u2018the most important and most daunting challenge humanity has ever faced.\\u2019 If we fail to meet this challenge, he concludes, malevolent or indifferent artificial intelligence (AI) will likely destroy us all.", "id": 39, "photo_url": "https://pe.tedcdn.com/images/ted/56aef93b63301ffe15f201c8b357ee2161abf955_254x191.jpg"}], "current_talk": 2243, "description": "Artificial intelligence is getting smarter by leaps and bounds -- within this century, research suggests, a computer AI could be as \\"smart\\" as a human being. And then, says Nick Bostrom, it will overtake us: \\"Machine intelligence is the last invention that humanity will ever need to make.\\" A philosopher and technologist, Bostrom asks us to think hard about the world we\'re building right now, driven by thinking machines. Will our smart machines help to preserve humanity and our values -- or will they have values of their own?", "language": "en", "url": "https://www.ted.com/talks/nick_bostrom_what_happens_when_our_computers_get_smarter_than_we_are", "media": {"internal": {"podcast-high-en": {"uri": "https://download.ted.com/talks/NickBostrom_2015-480p-en.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 118448253}, "podcast-low-en": {"uri": "https://download.ted.com/talks/NickBostrom_2015-low-en.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 22890094}, "podcast-high": {"uri": "https://download.ted.com/talks/NickBostrom_2015-480p.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 118449090}, "180k": {"uri": "https://download.ted.com/talks/NickBostrom_2015-180k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 22643582}, "64k": {"uri": "https://download.ted.com/talks/NickBostrom_2015-64k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 8194810}, "1500k": {"uri": "https://download.ted.com/talks/NickBostrom_2015-1500k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 183841327}, "450k": {"uri": "https://download.ted.com/talks/NickBostrom_2015-450k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 56455099}, "podcast-regular": {"uri": "https://download.ted.com/talks/NickBostrom_2015.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 56570463}, "950k": {"uri": "https://download.ted.com/talks/NickBostrom_2015-950k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 118456879}, "audio-podcast": {"uri": "https://download.ted.com/talks/NickBostrom_2015.mp3?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "audio/mp3", "filesize_bytes": 10228551}, "podcast-light": {"uri": "https://download.ted.com/talks/NickBostrom_2015-light.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 8328312}, "320k": {"uri": "https://download.ted.com/talks/NickBostrom_2015-320k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 40096064}, "600k": {"uri": "https://download.ted.com/talks/NickBostrom_2015-600k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 75463047}}}, "comments": {"count": 364, "id": 26863, "talk_id": 2243}, "slug": "nick_bostrom_what_happens_when_our_computers_get_smarter_than_we_are", "threadId": 26863, "talks": [{"event": "TED2015", "player_talks": [{"event": "TED2015", "slug": "nick_bostrom_what_happens_when_our_computers_get_smarter_than_we_are", "filmed": 1426636800, "targeting": {"event": "TED2015", "tag": "AI,future,machine learning,philosophy,technology", "id": 2243, "talk": "nick_bostrom_what_happens_when_our_computers_get_smarter_than_we_are", "year": "2015"}, "adDuration": "3.33", "external": null, "title": "What happens when our computers get smarter than we are?", "postAdDuration": "0.83", "published": 1430146817, "thumb": "https://pi.tedcdn.com/r/pe.tedcdn.com/images/ted/a693e3148df55358b76a30436f1accb09d1e2616_2880x1620.jpg?quality=89&w=600", "name": "Nick Bostrom: What happens when our computers get smarter than we are?", "languages": [{"languageCode": "ar", "endonym": "\\u0627\\u0644\\u0639\\u0631\\u0628\\u064a\\u0629", "isRtl": true, "ianaCode": "ar", "languageName": "Arabic"}, {"languageCode": "my", "endonym": "\\u1019\\u103c\\u1014\\u103a\\u1019\\u102c\\u1018\\u102c\\u101e\\u102c", "isRtl": false, "ianaCode": "my", "languageName": "Burmese"}, {"languageCode": "zh-cn", "endonym": "\\u4e2d\\u6587 (\\u7b80\\u4f53)", "isRtl": false, "ianaCode": "zh-Hans", "languageName": "Chinese, Simplified"}, {"languageCode": "zh-tw", "endonym": "\\u4e2d\\u6587 (\\u7e41\\u9ad4)", "isRtl": false, "ianaCode": "zh-Hant", "languageName": "Chinese, Traditional"}, {"languageCode": "hr", "endonym": "Hrvatski", "isRtl": false, "ianaCode": "hr", "languageName": "Croatian"}, {"languageCode": "cs", "endonym": "\\u010ce\\u0161tina", "isRtl": false, "ianaCode": "cs", "languageName": "Czech"}, {"languageCode": "nl", "endonym": "Nederlands", "isRtl": false, "ianaCode": "nl", "languageName": "Dutch"}, {"languageCode": "en", "endonym": "English", "isRtl": false, "ianaCode": "en", "languageName": "English"}, {"languageCode": "et", "endonym": "Eesti keel", "isRtl": false, "ianaCode": "et", "languageName": "Estonian"}, {"languageCode": "fr", "endonym": "Fran\\u00e7ais", "isRtl": false, "ianaCode": "fr", "languageName": "French"}, {"languageCode": "fr-ca", "endonym": "Fran\\u00e7ais canadien", "isRtl": false, "ianaCode": "fr-CA", "languageName": "French (Canada)"}, {"languageCode": "ka", "endonym": "\\u10e5\\u10d0\\u10e0\\u10d7\\u10e3\\u10da\\u10d8", "isRtl": false, "ianaCode": "ka", "languageName": "Georgian"}, {"languageCode": "el", "endonym": "\\u0395\\u03bb\\u03bb\\u03b7\\u03bd\\u03b9\\u03ba\\u03ac", "isRtl": false, "ianaCode": "el", "languageName": "Greek"}, {"languageCode": "he", "endonym": "\\u05e2\\u05d1\\u05e8\\u05d9\\u05ea", "isRtl": true, "ianaCode": "he", "languageName": "Hebrew"}, {"languageCode": "hu", "endonym": "Magyar", "isRtl": false, "ianaCode": "hu", "languageName": "Hungarian"}, {"languageCode": "it", "endonym": "Italiano", "isRtl": false, "ianaCode": "it", "languageName": "Italian"}, {"languageCode": "ja", "endonym": "\\u65e5\\u672c\\u8a9e", "isRtl": false, "ianaCode": "ja", "languageName": "Japanese"}, {"languageCode": "ko", "endonym": "\\ud55c\\uad6d\\uc5b4", "isRtl": false, "ianaCode": "ko", "languageName": "Korean"}, {"languageCode": "fa", "endonym": "\\u0641\\u0627\\u0631\\u0633\\u0649", "isRtl": true, "ianaCode": "fa", "languageName": "Persian"}, {"languageCode": "pt", "endonym": "Portugu\\u00eas de Portugal", "isRtl": false, "ianaCode": "pt", "languageName": "Portuguese"}, {"languageCode": "pt-br", "endonym": "Portugu\\u00eas brasileiro", "isRtl": false, "ianaCode": "pt-BR", "languageName": "Portuguese, Brazilian"}, {"languageCode": "ro", "endonym": "Rom\\u00e2n\\u0103", "isRtl": false, "ianaCode": "ro", "languageName": "Romanian"}, {"languageCode": "ru", "endonym": "\\u0420\\u0443\\u0441\\u0441\\u043a\\u0438\\u0439", "isRtl": false, "ianaCode": "ru", "languageName": "Russian"}, {"languageCode": "sr", "endonym": "\\u0421\\u0440\\u043f\\u0441\\u043a\\u0438, Srpski", "isRtl": false, "ianaCode": "sr", "languageName": "Serbian"}, {"languageCode": "es", "endonym": "Espa\\u00f1ol", "isRtl": false, "ianaCode": "es", "languageName": "Spanish"}, {"languageCode": "th", "endonym": "\\u0e20\\u0e32\\u0e29\\u0e32\\u0e44\\u0e17\\u0e22", "isRtl": false, "ianaCode": "th", "languageName": "Thai"}, {"languageCode": "tr", "endonym": "T\\u00fcrk\\u00e7e", "isRtl": false, "ianaCode": "tr", "languageName": "Turkish"}, {"languageCode": "uk", "endonym": "\\u0423\\u043a\\u0440\\u0430\\u0457\\u043d\\u0441\\u044c\\u043a\\u0430", "isRtl": false, "ianaCode": "uk", "languageName": "Ukrainian"}, {"languageCode": "vi", "endonym": "Ti\\u1ebfng Vi\\u1ec7t", "isRtl": false, "ianaCode": "vi", "languageName": "Vietnamese"}], "nativeLanguage": "en", "tags": ["AI", "future", "machine learning", "philosophy", "technology"], "speaker": "Nick Bostrom", "isSubtitleRequired": false, "introDuration": 11.82, "duration": 991, "id": 2243, "resources": {"h264": [{"bitrate": 320, "file": "https://download.ted.com/talks/NickBostrom_2015-320k.mp4?dnt"}], "hls": {"maiTargeting": {"event": "TED2015", "tag": "AI,future,machine learning,philosophy,technology", "id": 2243, "talk": "nick_bostrom_what_happens_when_our_computers_get_smarter_than_we_are", "year": "2015"}, "metadata": "https://hls.ted.com/talks/2243.json", "stream": "https://hls.ted.com/talks/2243.m3u8", "adUrl": "https://pubads.g.doubleclick.net/gampad/ads?ciu_szs=300x250%2C512x288%2C120x60%2C320x50%2C6x7%2C6x8&correlator=%5Bcorrelator%5D&cust_params=event%3DTED2015%26id%3D2243%26tag%3DAI%2Cfuture%2Cmachine%2Blearning%2Cphilosophy%2Ctechnology%26talk%3Dnick_bostrom_what_happens_when_our_computers_get_smarter_than_we_are%26year%3D2015&env=vp&gdfp_req=1&impl=s&iu=%2F5641%2Fmobile%2Fios%2Fweb&output=xml_vast2&sz=640x360&unviewed_position_start=1&url=%5Breferrer%5D"}}, "canonical": "https://www.ted.com/talks/nick_bostrom_what_happens_when_our_computers_get_smarter_than_we_are"}], "hero_load": "https://pi.tedcdn.com/r/pe.tedcdn.com/images/ted/a693e3148df55358b76a30436f1accb09d1e2616_2880x1620.jpg?q=50&w=15", "duration": 991, "id": 2243, "ratings": [{"count": 148, "id": 3, "name": "Courageous"}, {"count": 684, "id": 8, "name": "Informative"}, {"count": 316, "id": 10, "name": "Inspiring"}, {"count": 63, "id": 11, "name": "Longwinded"}, {"count": 80, "id": 21, "name": "Unconvincing"}, {"count": 13, "id": 26, "name": "Obnoxious"}, {"count": 130, "id": 25, "name": "OK"}, {"count": 25, "id": 7, "name": "Funny"}, {"count": 373, "id": 22, "name": "Fascinating"}, {"count": 218, "id": 24, "name": "Persuasive"}, {"count": 97, "id": 23, "name": "Jaw-dropping"}, {"count": 116, "id": 9, "name": "Ingenious"}, {"count": 45, "id": 2, "name": "Confusing"}, {"count": 27, "id": 1, "name": "Beautiful"}], "speakers": [{"description": "Philosopher", "firstname": "Nick", "title": "", "lastname": "Bostrom", "middleinitial": "", "whylisten": "<p>Philosopher Nick Bostrom envisioned a future full of human enhancement, nanotechnology and machine intelligence long before they became mainstream concerns. From his famous simulation argument -- which identified some striking implications of rejecting the Matrix-like idea that humans are living in a computer simulation -- to his work on existential risk, Bostrom approaches both the inevitable and the speculative using the tools of philosophy, probability theory, and scientific analysis.<br /><br />Since 2005, Bostrom has led the <a href=\\"http://www.fhi.ox.ac.uk/\\" target=\\"_blank\\">Future of Humanity Institute</a>, a research group of mathematicians, philosophers and scientists at Oxford University tasked with investigating the big picture for the human condition and its future. He has been referred to as one of the most important thinkers of our age.</p><p>Nick was honored as one of <em>Foreign Policy</em>&#39;s 2015&nbsp;<a href=\\"http://2015globalthinkers.foreignpolicy.com/?utm_source=Sailthru&amp;utm_medium=email&amp;utm_campaign=New%20Campaign&amp;utm_term=Flashpoints#!advocates/detail/bostrom\\" target=\\"_blank\\">Global Thinkers</a> .</p><p>His recent book <a href=\\"http://geni.us/superintelligence\\" target=\\"_blank\\"><em>Superintelligence</em></a>  advances the ominous idea that &ldquo;the first ultraintelligent machine is the last invention that man need ever make.&rdquo;</p>", "slug": "nick_bostrom", "whotheyare": "Nick Bostrom asks big questions: What should we do, as individuals and as a species, to optimize our long-term prospects? Will humanity\\u2019s technological advancements ultimately destroy us?", "whatotherssay": "Bostrom cogently argues that the prospect of superintelligent machines is \\u2018the most important and most daunting challenge humanity has ever faced.\\u2019 If we fail to meet this challenge, he concludes, malevolent or indifferent artificial intelligence (AI) will likely destroy us all.", "id": 39, "photo_url": "https://pe.tedcdn.com/images/ted/56aef93b63301ffe15f201c8b357ee2161abf955_254x191.jpg"}], "title": "What happens when our computers get smarter than we are?", "take_action": null, "comments": 26863, "more_resources": null, "hero": "https://pe.tedcdn.com/images/ted/a693e3148df55358b76a30436f1accb09d1e2616_2880x1620.jpg", "description": "Artificial intelligence is getting smarter by leaps and bounds -- within this century, research suggests, a computer AI could be as \\"smart\\" as a human being. And then, says Nick Bostrom, it will overtake us: \\"Machine intelligence is the last invention that humanity will ever need to make.\\" A philosopher and technologist, Bostrom asks us to think hard about the world we\'re building right now, driven by thinking machines. Will our smart machines help to preserve humanity and our values -- or will they have values of their own?", "tags": ["AI", "future", "machine learning", "philosophy", "technology"], "downloads": {"languages": [{"languageCode": "ar", "endonym": "\\u0627\\u0644\\u0639\\u0631\\u0628\\u064a\\u0629", "isRtl": true, "ianaCode": "ar", "languageName": "Arabic"}, {"languageCode": "my", "endonym": "\\u1019\\u103c\\u1014\\u103a\\u1019\\u102c\\u1018\\u102c\\u101e\\u102c", "isRtl": false, "ianaCode": "my", "languageName": "Burmese"}, {"languageCode": "zh-cn", "endonym": "\\u4e2d\\u6587 (\\u7b80\\u4f53)", "isRtl": false, "ianaCode": "zh-Hans", "languageName": "Chinese, Simplified"}, {"languageCode": "zh-tw", "endonym": "\\u4e2d\\u6587 (\\u7e41\\u9ad4)", "isRtl": false, "ianaCode": "zh-Hant", "languageName": "Chinese, Traditional"}, {"languageCode": "hr", "endonym": "Hrvatski", "isRtl": false, "ianaCode": "hr", "languageName": "Croatian"}, {"languageCode": "cs", "endonym": "\\u010ce\\u0161tina", "isRtl": false, "ianaCode": "cs", "languageName": "Czech"}, {"languageCode": "nl", "endonym": "Nederlands", "isRtl": false, "ianaCode": "nl", "languageName": "Dutch"}, {"languageCode": "en", "endonym": "English", "isRtl": false, "ianaCode": "en", "languageName": "English"}, {"languageCode": "et", "endonym": "Eesti keel", "isRtl": false, "ianaCode": "et", "languageName": "Estonian"}, {"languageCode": "fr", "endonym": "Fran\\u00e7ais", "isRtl": false, "ianaCode": "fr", "languageName": "French"}, {"languageCode": "fr-ca", "endonym": "Fran\\u00e7ais canadien", "isRtl": false, "ianaCode": "fr-CA", "languageName": "French (Canada)"}, {"languageCode": "ka", "endonym": "\\u10e5\\u10d0\\u10e0\\u10d7\\u10e3\\u10da\\u10d8", "isRtl": false, "ianaCode": "ka", "languageName": "Georgian"}, {"languageCode": "el", "endonym": "\\u0395\\u03bb\\u03bb\\u03b7\\u03bd\\u03b9\\u03ba\\u03ac", "isRtl": false, "ianaCode": "el", "languageName": "Greek"}, {"languageCode": "he", "endonym": "\\u05e2\\u05d1\\u05e8\\u05d9\\u05ea", "isRtl": true, "ianaCode": "he", "languageName": "Hebrew"}, {"languageCode": "hu", "endonym": "Magyar", "isRtl": false, "ianaCode": "hu", "languageName": "Hungarian"}, {"languageCode": "it", "endonym": "Italiano", "isRtl": false, "ianaCode": "it", "languageName": "Italian"}, {"languageCode": "ja", "endonym": "\\u65e5\\u672c\\u8a9e", "isRtl": false, "ianaCode": "ja", "languageName": "Japanese"}, {"languageCode": "ko", "endonym": "\\ud55c\\uad6d\\uc5b4", "isRtl": false, "ianaCode": "ko", "languageName": "Korean"}, {"languageCode": "fa", "endonym": "\\u0641\\u0627\\u0631\\u0633\\u0649", "isRtl": true, "ianaCode": "fa", "languageName": "Persian"}, {"languageCode": "pt", "endonym": "Portugu\\u00eas de Portugal", "isRtl": false, "ianaCode": "pt", "languageName": "Portuguese"}, {"languageCode": "pt-br", "endonym": "Portugu\\u00eas brasileiro", "isRtl": false, "ianaCode": "pt-BR", "languageName": "Portuguese, Brazilian"}, {"languageCode": "ro", "endonym": "Rom\\u00e2n\\u0103", "isRtl": false, "ianaCode": "ro", "languageName": "Romanian"}, {"languageCode": "ru", "endonym": "\\u0420\\u0443\\u0441\\u0441\\u043a\\u0438\\u0439", "isRtl": false, "ianaCode": "ru", "languageName": "Russian"}, {"languageCode": "sr", "endonym": "\\u0421\\u0440\\u043f\\u0441\\u043a\\u0438, Srpski", "isRtl": false, "ianaCode": "sr", "languageName": "Serbian"}, {"languageCode": "es", "endonym": "Espa\\u00f1ol", "isRtl": false, "ianaCode": "es", "languageName": "Spanish"}, {"languageCode": "th", "endonym": "\\u0e20\\u0e32\\u0e29\\u0e32\\u0e44\\u0e17\\u0e22", "isRtl": false, "ianaCode": "th", "languageName": "Thai"}, {"languageCode": "tr", "endonym": "T\\u00fcrk\\u00e7e", "isRtl": false, "ianaCode": "tr", "languageName": "Turkish"}, {"languageCode": "uk", "endonym": "\\u0423\\u043a\\u0440\\u0430\\u0457\\u043d\\u0441\\u044c\\u043a\\u0430", "isRtl": false, "ianaCode": "uk", "languageName": "Ukrainian"}, {"languageCode": "vi", "endonym": "Ti\\u1ebfng Vi\\u1ec7t", "isRtl": false, "ianaCode": "vi", "languageName": "Vietnamese"}], "subtitledDownloads": {"el": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-el.mp4", "name": "Greek", "low": "https://download.ted.com/talks/NickBostrom_2015-low-el.mp4"}, "en": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-en.mp4", "name": "English", "low": "https://download.ted.com/talks/NickBostrom_2015-low-en.mp4"}, "vi": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-vi.mp4", "name": "Vietnamese", "low": "https://download.ted.com/talks/NickBostrom_2015-low-vi.mp4"}, "it": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-it.mp4", "name": "Italian", "low": "https://download.ted.com/talks/NickBostrom_2015-low-it.mp4"}, "ar": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-ar.mp4", "name": "Arabic", "low": "https://download.ted.com/talks/NickBostrom_2015-low-ar.mp4"}, "pt-br": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-pt-br.mp4", "name": "Portuguese, Brazilian", "low": "https://download.ted.com/talks/NickBostrom_2015-low-pt-br.mp4"}, "cs": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-cs.mp4", "name": "Czech", "low": "https://download.ted.com/talks/NickBostrom_2015-low-cs.mp4"}, "et": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-et.mp4", "name": "Estonian", "low": "https://download.ted.com/talks/NickBostrom_2015-low-et.mp4"}, "es": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-es.mp4", "name": "Spanish", "low": "https://download.ted.com/talks/NickBostrom_2015-low-es.mp4"}, "ru": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-ru.mp4", "name": "Russian", "low": "https://download.ted.com/talks/NickBostrom_2015-low-ru.mp4"}, "nl": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-nl.mp4", "name": "Dutch", "low": "https://download.ted.com/talks/NickBostrom_2015-low-nl.mp4"}, "pt": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-pt.mp4", "name": "Portuguese", "low": "https://download.ted.com/talks/NickBostrom_2015-low-pt.mp4"}, "zh-cn": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-zh-cn.mp4", "name": "Chinese, Simplified", "low": "https://download.ted.com/talks/NickBostrom_2015-low-zh-cn.mp4"}, "zh-tw": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-zh-tw.mp4", "name": "Chinese, Traditional", "low": "https://download.ted.com/talks/NickBostrom_2015-low-zh-tw.mp4"}, "tr": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-tr.mp4", "name": "Turkish", "low": "https://download.ted.com/talks/NickBostrom_2015-low-tr.mp4"}, "fr-ca": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-fr-ca.mp4", "name": "French (Canada)", "low": "https://download.ted.com/talks/NickBostrom_2015-low-fr-ca.mp4"}, "th": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-th.mp4", "name": "Thai", "low": "https://download.ted.com/talks/NickBostrom_2015-low-th.mp4"}, "ro": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-ro.mp4", "name": "Romanian", "low": "https://download.ted.com/talks/NickBostrom_2015-low-ro.mp4"}, "fr": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-fr.mp4", "name": "French", "low": "https://download.ted.com/talks/NickBostrom_2015-low-fr.mp4"}, "hr": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-hr.mp4", "name": "Croatian", "low": "https://download.ted.com/talks/NickBostrom_2015-low-hr.mp4"}, "hu": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-hu.mp4", "name": "Hungarian", "low": "https://download.ted.com/talks/NickBostrom_2015-low-hu.mp4"}, "fa": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-fa.mp4", "name": "Persian", "low": "https://download.ted.com/talks/NickBostrom_2015-low-fa.mp4"}, "ja": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-ja.mp4", "name": "Japanese", "low": "https://download.ted.com/talks/NickBostrom_2015-low-ja.mp4"}, "he": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-he.mp4", "name": "Hebrew", "low": "https://download.ted.com/talks/NickBostrom_2015-low-he.mp4"}, "ka": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-ka.mp4", "name": "Georgian", "low": "https://download.ted.com/talks/NickBostrom_2015-low-ka.mp4"}, "sr": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-sr.mp4", "name": "Serbian", "low": "https://download.ted.com/talks/NickBostrom_2015-low-sr.mp4"}, "ko": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-ko.mp4", "name": "Korean", "low": "https://download.ted.com/talks/NickBostrom_2015-low-ko.mp4"}, "uk": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-uk.mp4", "name": "Ukrainian", "low": "https://download.ted.com/talks/NickBostrom_2015-low-uk.mp4"}, "my": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p-my.mp4", "name": "Burmese", "low": "https://download.ted.com/talks/NickBostrom_2015-low-my.mp4"}}, "nativeDownloads": {"high": "https://download.ted.com/talks/NickBostrom_2015-480p.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "medium": "https://download.ted.com/talks/NickBostrom_2015.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "low": "https://download.ted.com/talks/NickBostrom_2015-light.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22"}, "id": 2243, "audioDownload": "https://download.ted.com/talks/NickBostrom_2015.mp3?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22"}, "related_talks": [{"viewed_count": 1145570, "hero": "https://pe.tedcdn.com/images/ted/c1c6ba11c2170e47c9cae5e674edfd102a5b85f3_1600x1200.jpg", "title": "Watson, Jeopardy and me, the obsolete know-it-all", "id": 1708, "speaker": "Ken Jennings", "duration": 1072, "slug": "ken_jennings_watson_jeopardy_and_me_the_obsolete_know_it_all"}, {"viewed_count": 2034864, "hero": "https://pe.tedcdn.com/images/ted/fbada01990f86f5afa850cc23a0259fec091f929_2880x1620.jpg", "title": "How we\'re teaching computers to understand pictures", "id": 2218, "speaker": "Fei-Fei Li", "duration": 1078, "slug": "fei_fei_li_how_we_re_teaching_computers_to_understand_pictures"}, {"viewed_count": 2208071, "hero": "https://pe.tedcdn.com/images/ted/4707d8e88ba824e4a9ad05ee2446d93576117d21_2880x1620.jpg", "title": "The wonderful and terrifying implications of computers that can learn", "id": 2155, "speaker": "Jeremy Howard", "duration": 1185, "slug": "jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn"}, {"viewed_count": 2700812, "hero": "https://pe.tedcdn.com/images/ted/eda8b85771e90749c3e3e072c3f319417713ee92_2880x1620.jpg", "title": "Can we build AI without losing control over it?", "id": 2592, "speaker": "Sam Harris", "duration": 867, "slug": "sam_harris_can_we_build_ai_without_losing_control_over_it"}, {"viewed_count": 1982362, "hero": "https://pe.tedcdn.com/images/ted/15af4df75aba589bfcd57c1e72b572e24067bc29_1600x1200.jpg", "title": "A new equation for intelligence", "id": 1922, "speaker": "Alex Wissner-Gross", "duration": 708, "slug": "alex_wissner_gross_a_new_equation_for_intelligence"}, {"viewed_count": 1409068, "hero": "https://pe.tedcdn.com/images/ted/1879eda9d3817eaadc472c49075a618eaf4b4100_2880x1620.jpg", "title": "How AI can bring on a second Industrial Revolution", "id": 2645, "speaker": "Kevin Kelly", "duration": 824, "slug": "kevin_kelly_how_ai_can_bring_on_a_second_industrial_revolution"}], "recorded_at": "2015-03-18T00:00:00.000+00:00", "slug": "nick_bostrom_what_happens_when_our_computers_get_smarter_than_we_are", "speaker_name": "Nick Bostrom", "viewed_count": 2897720, "event_badge": null, "event_blurb": "This talk was presented at an official TED conference, and was featured by our editors on the home page.", "recommendations": null, "corrections": null}], "event": "TED2015", "name": "Nick Bostrom: What happens when our computers get smarter than we are?"}'
p363
sS'keywords'
p364
(lp365
VAI
p366
aVfuture
p367
aVmachine learning
p368
aVphilosophy
p369
aVtechnology
p370
asS'datecrawled'
p371
g358
(S'\x07\xe1\n\x17\x02\x1e;\x03\xc0\xa5'
tRp372
sS'id'
p373
I2243
ss.