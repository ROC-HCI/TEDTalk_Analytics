(dp1
S'talk_transcript'
p2
(lp3
(lp4
VPower.
p5
aVThat is the word that comes to mind.
p6
aVWe're the new technologists.
p7
aVWe have a lot of data, so we have a lot of power.
p8
aVHow much power do we have?
p9
aVScene from a movie: "Apocalypse Now" \u2014 great movie.
p10
aVWe've got to get our hero, Captain Willard, to the mouth of the Nung River
p11
aVso he can go pursue Colonel Kurtz.
p12
aVThe way we're going to do this is fly him in and drop him off.
p13
aVSo the scene:
p14
aVthe sky is filled with this fleet of helicopters carrying him in.
p15
aVAnd there's this loud, thrilling music in the background,
p16
aVthis wild music.
p17
aV\u266b Dum da ta da dum \u266b
p18
aV\u266b Dum da ta da dum \u266b
p19
aV\u266b Da ta da da \u266b
p20
aVThat's a lot of power.
p21
aVThat's the kind of power I feel in this room.
p22
aVThat's the kind of power we have
p23
aVbecause of all of the data that we have.
p24
aa(lp25
VLet's take an example.
p26
aVWhat can we do
p27
aVwith just one person's data?
p28
aVWhat can we do
p29
aVwith that guy's data?
p30
aVI can look at your financial records.
p31
aVI can tell if you pay your bills on time.
p32
aVI know if you're good to give a loan to.
p33
aVI can look at your medical records; I can see if your pump is still pumping \u2014
p34
aVsee if you're good to offer insurance to.
p35
aVI can look at your clicking patterns.
p36
aVWhen you come to my website, I actually know what you're going to do already
p37
aVbecause I've seen you visit millions of websites before.
p38
aVAnd I'm sorry to tell you,
p39
aVyou're like a poker player, you have a tell.
p40
aVI can tell with data analysis what you're going to do
p41
aVbefore you even do it.
p42
aVI know what you like. I know who you are,
p43
aVand that's even before I look at your mail
p44
aVor your phone.
p45
aa(lp46
VThose are the kinds of things we can do
p47
aVwith the data that we have.
p48
aVBut I'm not actually here to talk about what we can do.
p49
aVI'm here to talk about what we should do.
p50
aVWhat's the right thing to do?
p51
aa(lp52
VNow I see some puzzled looks
p53
aVlike, "Why are you asking us what's the right thing to do?
p54
aVWe're just building this stuff. Somebody else is using it."
p55
aVFair enough.
p56
aVBut it brings me back.
p57
aVI think about World War II \u2014
p58
aVsome of our great technologists then,
p59
aVsome of our great physicists,
p60
aVstudying nuclear fission and fusion \u2014
p61
aVjust nuclear stuff.
p62
aVWe gather together these physicists in Los Alamos
p63
aVto see what they'll build.
p64
aVWe want the people building the technology
p65
aVthinking about what we should be doing with the technology.
p66
aa(lp67
VSo what should we be doing with that guy's data?
p68
aVShould we be collecting it, gathering it,
p69
aVso we can make his online experience better?
p70
aVSo we can make money?
p71
aVSo we can protect ourselves
p72
aVif he was up to no good?
p73
aVOr should we respect his privacy,
p74
aVprotect his dignity and leave him alone?
p75
aVWhich one is it?
p76
aVHow should we figure it out?
p77
aa(lp78
VI know: crowdsource. Let's crowdsource this.
p79
aVSo to get people warmed up,
p80
aVlet's start with an easy question \u2014
p81
aVsomething I'm sure everybody here has an opinion about:
p82
aViPhone versus Android.
p83
aVLet's do a show of hands \u2014 iPhone.
p84
aVUh huh.
p85
aVAndroid.
p86
aVYou'd think with a bunch of smart people
p87
aVwe wouldn't be such suckers just for the pretty phones.
p88
aV(Laughter)
p89
aVNext question,
p90
aVa little bit harder.
p91
aVShould we be collecting all of that guy's data
p92
aVto make his experiences better
p93
aVand to protect ourselves in case he's up to no good?
p94
aVOr should we leave him alone?
p95
aVCollect his data.
p96
aVLeave him alone.
p97
aVYou're safe. It's fine.
p98
aV(Laughter)
p99
aVOkay, last question \u2014
p100
aVharder question \u2014
p101
aVwhen trying to evaluate
p102
aVwhat we should do in this case,
p103
aVshould we use a Kantian deontological moral framework,
p104
aVor should we use a Millian consequentialist one?
p105
aVKant.
p106
aVMill.
p107
aVNot as many votes.
p108
aV(Laughter)
p109
aVYeah, that's a terrifying result.
p110
aVTerrifying, because we have stronger opinions
p111
aVabout our hand-held devices
p112
aVthan about the moral framework
p113
aVwe should use to guide our decisions.
p114
aa(lp115
VHow do we know what to do with all the power we have
p116
aVif we don't have a moral framework?
p117
aVWe know more about mobile operating systems,
p118
aVbut what we really need is a moral operating system.
p119
aVWhat's a moral operating system?
p120
aVWe all know right and wrong, right?
p121
aVYou feel good when you do something right,
p122
aVyou feel bad when you do something wrong.
p123
aVOur parents teach us that: praise with the good, scold with the bad.
p124
aVBut how do we figure out what's right and wrong?
p125
aVAnd from day to day, we have the techniques that we use.
p126
aVMaybe we just follow our gut.
p127
aVMaybe we take a vote \u2014 we crowdsource.
p128
aVOr maybe we punt \u2014
p129
aVask the legal department, see what they say.
p130
aVIn other words, it's kind of random,
p131
aVkind of ad hoc,
p132
aVhow we figure out what we should do.
p133
aVAnd maybe, if we want to be on surer footing,
p134
aVwhat we really want is a moral framework that will help guide us there,
p135
aVthat will tell us what kinds of things are right and wrong in the first place,
p136
aVand how would we know in a given situation what to do.
p137
aa(lp138
VSo let's get a moral framework.
p139
aVWe're numbers people, living by numbers.
p140
aVHow can we use numbers
p141
aVas the basis for a moral framework?
p142
aVI know a guy who did exactly that.
p143
aVA brilliant guy \u2014
p144
aVhe's been dead 2,500 years.
p145
aVPlato, that's right.
p146
aVRemember him \u2014 old philosopher?
p147
aVYou were sleeping during that class.
p148
aVAnd Plato, he had a lot of the same concerns that we did.
p149
aVHe was worried about right and wrong.
p150
aVHe wanted to know what is just.
p151
aVBut he was worried that all we seem to be doing
p152
aVis trading opinions about this.
p153
aVHe says something's just. She says something else is just.
p154
aVIt's kind of convincing when he talks and when she talks too.
p155
aVI'm just going back and forth; I'm not getting anywhere.
p156
aVI don't want opinions; I want knowledge.
p157
aVI want to know the truth about justice \u2014
p158
aVlike we have truths in math.
p159
aVIn math, we know the objective facts.
p160
aVTake a number, any number \u2014 two.
p161
aVFavorite number. I love that number.
p162
aVThere are truths about two.
p163
aVIf you've got two of something,
p164
aVyou add two more, you get four.
p165
aVThat's true no matter what thing you're talking about.
p166
aVIt's an objective truth about the form of two,
p167
aVthe abstract form.
p168
aVWhen you have two of anything \u2014 two eyes, two ears, two noses,
p169
aVjust two protrusions \u2014
p170
aVthose all partake of the form of two.
p171
aVThey all participate in the truths that two has.
p172
aVThey all have two-ness in them.
p173
aVAnd therefore, it's not a matter of opinion.
p174
aa(lp175
VWhat if, Plato thought,
p176
aVethics was like math?
p177
aVWhat if there were a pure form of justice?
p178
aVWhat if there are truths about justice,
p179
aVand you could just look around in this world
p180
aVand see which things participated,
p181
aVpartook of that form of justice?
p182
aVThen you would know what was really just and what wasn't.
p183
aVIt wouldn't be a matter
p184
aVof just opinion or just appearances.
p185
aVThat's a stunning vision.
p186
aVI mean, think about that. How grand. How ambitious.
p187
aVThat's as ambitious as we are.
p188
aVHe wants to solve ethics.
p189
aVHe wants objective truths.
p190
aVIf you think that way,
p191
aVyou have a Platonist moral framework.
p192
aa(lp193
VIf you don't think that way,
p194
aVwell, you have a lot of company in the history of Western philosophy,
p195
aVbecause the tidy idea, you know, people criticized it.
p196
aVAristotle, in particular, he was not amused.
p197
aVHe thought it was impractical.
p198
aVAristotle said, "We should seek only so much precision in each subject
p199
aVas that subject allows."
p200
aVAristotle thought ethics wasn't a lot like math.
p201
aVHe thought ethics was a matter of making decisions in the here-and-now
p202
aVusing our best judgment
p203
aVto find the right path.
p204
aVIf you think that, Plato's not your guy.
p205
aVBut don't give up.
p206
aVMaybe there's another way
p207
aVthat we can use numbers as the basis of our moral framework.
p208
aa(lp209
VHow about this:
p210
aVWhat if in any situation you could just calculate,
p211
aVlook at the choices,
p212
aVmeasure out which one's better and know what to do?
p213
aVThat sound familiar?
p214
aVThat's a utilitarian moral framework.
p215
aVJohn Stuart Mill was a great advocate of this \u2014
p216
aVnice guy besides \u2014
p217
aVand only been dead 200 years.
p218
aVSo basis of utilitarianism \u2014
p219
aVI'm sure you're familiar at least.
p220
aVThe three people who voted for Mill before are familiar with this.
p221
aVBut here's the way it works.
p222
aVWhat if morals, what if what makes something moral
p223
aVis just a matter of if it maximizes pleasure
p224
aVand minimizes pain?
p225
aVIt does something intrinsic to the act.
p226
aVIt's not like its relation to some abstract form.
p227
aVIt's just a matter of the consequences.
p228
aVYou just look at the consequences
p229
aVand see if, overall, it's for the good or for the worse.
p230
aVThat would be simple. Then we know what to do.
p231
aa(lp232
VLet's take an example.
p233
aVSuppose I go up
p234
aVand I say, "I'm going to take your phone."
p235
aVNot just because it rang earlier,
p236
aVbut I'm going to take it because I made a little calculation.
p237
aVI thought, that guy looks suspicious.
p238
aVAnd what if he's been sending little messages to Bin Laden's hideout \u2014
p239
aVor whoever took over after Bin Laden \u2014
p240
aVand he's actually like a terrorist, a sleeper cell.
p241
aVI'm going to find that out, and when I find that out,
p242
aVI'm going to prevent a huge amount of damage that he could cause.
p243
aVThat has a very high utility to prevent that damage.
p244
aVAnd compared to the little pain that it's going to cause \u2014
p245
aVbecause it's going to be embarrassing when I'm looking on his phone
p246
aVand seeing that he has a Farmville problem and that whole bit \u2014
p247
aVthat's overwhelmed
p248
aVby the value of looking at the phone.
p249
aVIf you feel that way,
p250
aVthat's a utilitarian choice.
p251
aa(lp252
VBut maybe you don't feel that way either.
p253
aVMaybe you think, it's his phone.
p254
aVIt's wrong to take his phone
p255
aVbecause he's a person
p256
aVand he has rights and he has dignity,
p257
aVand we can't just interfere with that.
p258
aVHe has autonomy.
p259
aVIt doesn't matter what the calculations are.
p260
aVThere are things that are intrinsically wrong \u2014
p261
aVlike lying is wrong,
p262
aVlike torturing innocent children is wrong.
p263
aVKant was very good on this point,
p264
aVand he said it a little better than I'll say it.
p265
aVHe said we should use our reason
p266
aVto figure out the rules by which we should guide our conduct,
p267
aVand then it is our duty to follow those rules.
p268
aVIt's not a matter of calculation.
p269
aa(lp270
VSo let's stop.
p271
aVWe're right in the thick of it, this philosophical thicket.
p272
aVAnd this goes on for thousands of years,
p273
aVbecause these are hard questions,
p274
aVand I've only got 15 minutes.
p275
aVSo let's cut to the chase.
p276
aVHow should we be making our decisions?
p277
aVIs it Plato, is it Aristotle, is it Kant, is it Mill?
p278
aVWhat should we be doing? What's the answer?
p279
aVWhat's the formula that we can use in any situation
p280
aVto determine what we should do,
p281
aVwhether we should use that guy's data or not?
p282
aVWhat's the formula?
p283
aVThere's not a formula.
p284
aVThere's not a simple answer.
p285
aa(lp286
VEthics is hard.
p287
aVEthics requires thinking.
p288
aVAnd that's uncomfortable.
p289
aVI know; I spent a lot of my career
p290
aVin artificial intelligence,
p291
aVtrying to build machines that could do some of this thinking for us,
p292
aVthat could give us answers.
p293
aVBut they can't.
p294
aVYou can't just take human thinking
p295
aVand put it into a machine.
p296
aVWe're the ones who have to do it.
p297
aVHappily, we're not machines, and we can do it.
p298
aVNot only can we think,
p299
aVwe must.
p300
aVHannah Arendt said,
p301
aV"The sad truth
p302
aVis that most evil done in this world
p303
aVis not done by people
p304
aVwho choose to be evil.
p305
aVIt arises from not thinking."
p306
aVThat's what she called the "banality of evil."
p307
aVAnd the response to that
p308
aVis that we demand the exercise of thinking
p309
aVfrom every sane person.
p310
aa(lp311
VSo let's do that. Let's think.
p312
aVIn fact, let's start right now.
p313
aVEvery person in this room do this:
p314
aVthink of the last time you had a decision to make
p315
aVwhere you were worried to do the right thing,
p316
aVwhere you wondered, "What should I be doing?"
p317
aVBring that to mind,
p318
aVand now reflect on that
p319
aVand say, "How did I come up that decision?
p320
aVWhat did I do? Did I follow my gut?
p321
aVDid I have somebody vote on it? Or did I punt to legal?"
p322
aVOr now we have a few more choices.
p323
aV"Did I evaluate what would be the highest pleasure
p324
aVlike Mill would?
p325
aVOr like Kant, did I use reason to figure out what was intrinsically right?"
p326
aVThink about it. Really bring it to mind. This is important.
p327
aVIt is so important
p328
aVwe are going to spend 30 seconds of valuable TEDTalk time
p329
aVdoing nothing but thinking about this.
p330
aVAre you ready? Go.
p331
aa(lp332
VStop. Good work.
p333
aVWhat you just did,
p334
aVthat's the first step towards taking responsibility
p335
aVfor what we should do with all of our power.
p336
aa(lp337
VNow the next step \u2014 try this.
p338
aVGo find a friend and explain to them
p339
aVhow you made that decision.
p340
aVNot right now. Wait till I finish talking.
p341
aVDo it over lunch.
p342
aVAnd don't just find another technologist friend;
p343
aVfind somebody different than you.
p344
aVFind an artist or a writer \u2014
p345
aVor, heaven forbid, find a philosopher and talk to them.
p346
aVIn fact, find somebody from the humanities.
p347
aVWhy? Because they think about problems
p348
aVdifferently than we do as technologists.
p349
aVJust a few days ago, right across the street from here,
p350
aVthere was hundreds of people gathered together.
p351
aVIt was technologists and humanists
p352
aVat that big BiblioTech Conference.
p353
aVAnd they gathered together
p354
aVbecause the technologists wanted to learn
p355
aVwhat it would be like to think from a humanities perspective.
p356
aVYou have someone from Google
p357
aVtalking to someone who does comparative literature.
p358
aVYou're thinking about the relevance of 17th century French theater \u2014
p359
aVhow does that bear upon venture capital?
p360
aVWell that's interesting. That's a different way of thinking.
p361
aVAnd when you think in that way,
p362
aVyou become more sensitive to the human considerations,
p363
aVwhich are crucial to making ethical decisions.
p364
aa(lp365
VSo imagine that right now
p366
aVyou went and you found your musician friend.
p367
aVAnd you're telling him what we're talking about,
p368
aVabout our whole data revolution and all this \u2014
p369
aVmaybe even hum a few bars of our theme music.
p370
aV\u266b Dum ta da da dum dum ta da da dum \u266b
p371
aVWell, your musician friend will stop you and say,
p372
aV"You know, the theme music
p373
aVfor your data revolution,
p374
aVthat's an opera, that's Wagner.
p375
aVIt's based on Norse legend.
p376
aVIt's Gods and mythical creatures
p377
aVfighting over magical jewelry."
p378
aVThat's interesting.
p379
aVNow it's also a beautiful opera,
p380
aVand we're moved by that opera.
p381
aVWe're moved because it's about the battle
p382
aVbetween good and evil,
p383
aVabout right and wrong.
p384
aVAnd we care about right and wrong.
p385
aVWe care what happens in that opera.
p386
aVWe care what happens in "Apocalypse Now."
p387
aVAnd we certainly care
p388
aVwhat happens with our technologies.
p389
aa(lp390
VWe have so much power today,
p391
aVit is up to us to figure out what to do,
p392
aVand that's the good news.
p393
aVWe're the ones writing this opera.
p394
aVThis is our movie.
p395
aVWe figure out what will happen with this technology.
p396
aVWe determine how this will all end.
p397
aa(lp398
VThank you.
p399
aa(lp400
V(Applause)
p401
aasS'transcript_micsec'
p402
(lp403
I11000
aI56000
aI101000
aI120000
aI157000
aI183000
aI280000
aI342000
aI429000
aI470000
aI509000
aI558000
aI606000
aI647000
aI687000
aI745000
aI809000
aI821000
aI885000
aI942000
aI960000
aI962000
asS'talk_meta'
p404
(dp405
S'ratings'
p406
(dp407
S'ingenious'
p408
I37
sS'beautiful'
p409
I26
sS'inspiring'
p410
I291
sS'ok'
p411
I109
sS'fascinating'
p412
I109
sS'funny'
p413
I22
sS'total_count'
p414
I1429
sS'persuasive'
p415
I227
sS'longwinded'
p416
I102
sS'unconvincing'
p417
I123
sS'informative'
p418
I177
sS'jaw-dropping'
p419
I19
sS'obnoxious'
p420
I76
sS'courageous'
p421
I77
sS'confusing'
p422
I34
ssS'author'
p423
VDamon_Horowitz;
p424
sS'url'
p425
S'https://www.ted.com/talks/damon_horowitz'
p426
sS'vidlen'
p427
I978
sS'totalviews'
p428
I685794
sS'title'
p429
VWe need a "moral operating system"
p430
sS'downloadlink'
p431
Vhttps://download.ted.com/talks/DamonHorowitz_2011X.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22
p432
sS'datepublished'
p433
cdatetime
datetime
p434
(S'\x07\xdb\x06\x06\n&\x00\x00\x00\x00'
tRp435
sS'datefilmed'
p436
g434
(S'\x07\xdb\x06\x06\n&\x00\x00\x00\x00'
tRp437
sS'alldata_JSON'
p438
S'{"viewed_count": 685794, "speakers": [{"description": "Philosopher, entrepreneur", "firstname": "Damon", "title": "", "lastname": "Horowitz", "middleinitial": "", "whylisten": "<p>Damon Horowitz is a philosophy professor and serial entrepreneur. He recently joined Google as In-House Philosopher / Director of Engineering, heading development of several initiatives involving social and search. He came to Google from Aardvark, the social search engine, where he was co-founder and CTO, overseeing product development and research strategy. Prior to Aardvark, Horowitz built several companies around applications of intelligent language processing. He co-founded Perspecta (acquired by Excite), was lead architect for Novation Biosciences (acquired by Agilent), and co-founded NewsDB (now Daylife). </p><p>Horowitz teaches courses in philosophy, cognitive science, and computer science at several institutions, including Stanford, NYU, University of Pennsylvania and <a href=\\"http://www.prisonuniversityproject.org/\\" target=\\"_blank\\">San Quentin State Prison</a>. </p><p>Get more information on the <a href=\\"http://www.prisonuniversityproject.org/\\" target=\\"_blank\\">Prison University Project &gt;&gt;</a>  </p>", "slug": "damon_horowitz", "whotheyare": "Damon Horowitz explores what is possible at the boundaries of technology and the humanities.", "whatotherssay": "Damon seems to have realized in reverse order that the horse belongs in front of the carriage -- that humanity should lead technology, not vice versa.", "id": 1062, "photo_url": "https://pe.tedcdn.com/images/ted/266bebcd762dca63b6bc5cb255647c96fc475bf4_254x191.jpg"}], "current_talk": 1162, "description": "Damon Horowitz reviews the enormous new powers that technology gives us: to know more -- and more about each other -- than ever before. Drawing the audience into a philosophical discussion, Horowitz invites us to pay new attention to the basic philosophy -- the ethical principles -- behind the burst of invention remaking our world. Where\'s the moral operating system that allows us to make sense of it?", "language": "en", "url": "https://www.ted.com/talks/damon_horowitz", "media": {"internal": {"podcast-high-en": {"uri": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-en.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 113545497}, "podcast-low-en": {"uri": "https://download.ted.com/talks/DamonHorowitz_2011X-low-en.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 22371924}, "podcast-high": {"uri": "https://download.ted.com/talks/DamonHorowitz_2011X-480p.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 113872799}, "180k": {"uri": "https://download.ted.com/talks/DamonHorowitz_2011X-180k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 22255855}, "64k": {"uri": "https://download.ted.com/talks/DamonHorowitz_2011X-64k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 8014008}, "1500k": {"uri": "https://download.ted.com/talks/DamonHorowitz_2011X-1500k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 177964690}, "450k": {"uri": "https://download.ted.com/talks/DamonHorowitz_2011X-450k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 55198754}, "podcast-regular": {"uri": "https://download.ted.com/talks/DamonHorowitz_2011X.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 55835674}, "950k": {"uri": "https://download.ted.com/talks/DamonHorowitz_2011X-950k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 113257215}, "podcast-light": {"uri": "https://download.ted.com/talks/DamonHorowitz_2011X-light.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 8476146}, "320k": {"uri": "https://download.ted.com/talks/DamonHorowitz_2011X-320k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 39349759}, "600k": {"uri": "https://download.ted.com/talks/DamonHorowitz_2011X-600k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 73410308}}}, "comments": {"count": 136, "id": 4476, "talk_id": 1162}, "slug": "damon_horowitz", "threadId": 4476, "talks": [{"event": "TEDxSiliconValley", "player_talks": [{"event": "TEDxSiliconValley", "slug": "damon_horowitz", "filmed": 1305331200, "targeting": {"event": "TEDxSiliconValley", "tag": "TEDx,culture,philosophy,technology", "id": 1162, "talk": "damon_horowitz", "year": "2011"}, "adDuration": "3.33", "external": null, "title": "We need a \\"moral operating system\\"", "postAdDuration": "0.83", "published": 1307371080, "thumb": "https://pi.tedcdn.com/r/pe.tedcdn.com/images/ted/aa72305d473525c3179cdc471b3ef24a90c9a630_2880x1620.jpg?quality=89&w=600", "name": "Damon Horowitz: We need a \\"moral operating system\\"", "languages": [{"languageCode": "sq", "endonym": "Shqip", "isRtl": false, "ianaCode": "sq", "languageName": "Albanian"}, {"languageCode": "ar", "endonym": "\\u0627\\u0644\\u0639\\u0631\\u0628\\u064a\\u0629", "isRtl": true, "ianaCode": "ar", "languageName": "Arabic"}, {"languageCode": "bg", "endonym": "\\u0431\\u044a\\u043b\\u0433\\u0430\\u0440\\u0441\\u043a\\u0438", "isRtl": false, "ianaCode": "bg", "languageName": "Bulgarian"}, {"languageCode": "zh-cn", "endonym": "\\u4e2d\\u6587 (\\u7b80\\u4f53)", "isRtl": false, "ianaCode": "zh-Hans", "languageName": "Chinese, Simplified"}, {"languageCode": "zh-tw", "endonym": "\\u4e2d\\u6587 (\\u7e41\\u9ad4)", "isRtl": false, "ianaCode": "zh-Hant", "languageName": "Chinese, Traditional"}, {"languageCode": "hr", "endonym": "Hrvatski", "isRtl": false, "ianaCode": "hr", "languageName": "Croatian"}, {"languageCode": "cs", "endonym": "\\u010ce\\u0161tina", "isRtl": false, "ianaCode": "cs", "languageName": "Czech"}, {"languageCode": "nl", "endonym": "Nederlands", "isRtl": false, "ianaCode": "nl", "languageName": "Dutch"}, {"languageCode": "en", "endonym": "English", "isRtl": false, "ianaCode": "en", "languageName": "English"}, {"languageCode": "fr", "endonym": "Fran\\u00e7ais", "isRtl": false, "ianaCode": "fr", "languageName": "French"}, {"languageCode": "de", "endonym": "Deutsch", "isRtl": false, "ianaCode": "de", "languageName": "German"}, {"languageCode": "el", "endonym": "\\u0395\\u03bb\\u03bb\\u03b7\\u03bd\\u03b9\\u03ba\\u03ac", "isRtl": false, "ianaCode": "el", "languageName": "Greek"}, {"languageCode": "he", "endonym": "\\u05e2\\u05d1\\u05e8\\u05d9\\u05ea", "isRtl": true, "ianaCode": "he", "languageName": "Hebrew"}, {"languageCode": "hu", "endonym": "Magyar", "isRtl": false, "ianaCode": "hu", "languageName": "Hungarian"}, {"languageCode": "id", "endonym": "Bahasa Indonesia", "isRtl": false, "ianaCode": "id", "languageName": "Indonesian"}, {"languageCode": "it", "endonym": "Italiano", "isRtl": false, "ianaCode": "it", "languageName": "Italian"}, {"languageCode": "ja", "endonym": "\\u65e5\\u672c\\u8a9e", "isRtl": false, "ianaCode": "ja", "languageName": "Japanese"}, {"languageCode": "ko", "endonym": "\\ud55c\\uad6d\\uc5b4", "isRtl": false, "ianaCode": "ko", "languageName": "Korean"}, {"languageCode": "nb", "endonym": "Norsk bokm\\u00e5l", "isRtl": false, "ianaCode": "nb", "languageName": "Norwegian Bokmal"}, {"languageCode": "fa", "endonym": "\\u0641\\u0627\\u0631\\u0633\\u0649", "isRtl": true, "ianaCode": "fa", "languageName": "Persian"}, {"languageCode": "pl", "endonym": "Polski", "isRtl": false, "ianaCode": "pl", "languageName": "Polish"}, {"languageCode": "pt", "endonym": "Portugu\\u00eas de Portugal", "isRtl": false, "ianaCode": "pt", "languageName": "Portuguese"}, {"languageCode": "pt-br", "endonym": "Portugu\\u00eas brasileiro", "isRtl": false, "ianaCode": "pt-BR", "languageName": "Portuguese, Brazilian"}, {"languageCode": "ro", "endonym": "Rom\\u00e2n\\u0103", "isRtl": false, "ianaCode": "ro", "languageName": "Romanian"}, {"languageCode": "ru", "endonym": "\\u0420\\u0443\\u0441\\u0441\\u043a\\u0438\\u0439", "isRtl": false, "ianaCode": "ru", "languageName": "Russian"}, {"languageCode": "sr", "endonym": "\\u0421\\u0440\\u043f\\u0441\\u043a\\u0438, Srpski", "isRtl": false, "ianaCode": "sr", "languageName": "Serbian"}, {"languageCode": "es", "endonym": "Espa\\u00f1ol", "isRtl": false, "ianaCode": "es", "languageName": "Spanish"}, {"languageCode": "th", "endonym": "\\u0e20\\u0e32\\u0e29\\u0e32\\u0e44\\u0e17\\u0e22", "isRtl": false, "ianaCode": "th", "languageName": "Thai"}, {"languageCode": "tr", "endonym": "T\\u00fcrk\\u00e7e", "isRtl": false, "ianaCode": "tr", "languageName": "Turkish"}, {"languageCode": "vi", "endonym": "Ti\\u1ebfng Vi\\u1ec7t", "isRtl": false, "ianaCode": "vi", "languageName": "Vietnamese"}], "nativeLanguage": "en", "tags": ["TEDx", "culture", "philosophy", "technology"], "speaker": "Damon Horowitz", "isSubtitleRequired": false, "introDuration": 11.82, "duration": 978, "id": 1162, "resources": {"h264": [{"bitrate": 320, "file": "https://download.ted.com/talks/DamonHorowitz_2011X-320k.mp4?dnt"}], "hls": {"maiTargeting": {"event": "TEDxSiliconValley", "tag": "TEDx,culture,philosophy,technology", "id": 1162, "talk": "damon_horowitz", "year": "2011"}, "metadata": "https://hls.ted.com/talks/1162.json", "stream": "https://hls.ted.com/talks/1162.m3u8", "adUrl": "https://pubads.g.doubleclick.net/gampad/ads?ciu_szs=300x250%2C512x288%2C120x60%2C320x50%2C6x7%2C6x8&correlator=%5Bcorrelator%5D&cust_params=event%3DTEDxSiliconValley%26id%3D1162%26tag%3DTEDx%2Cculture%2Cphilosophy%2Ctechnology%26talk%3Ddamon_horowitz%26year%3D2011&env=vp&gdfp_req=1&impl=s&iu=%2F5641%2Fmobile%2Fios%2Fweb&output=xml_vast2&sz=640x360&unviewed_position_start=1&url=%5Breferrer%5D"}}, "canonical": "https://www.ted.com/talks/damon_horowitz"}], "hero_load": "https://pi.tedcdn.com/r/pe.tedcdn.com/images/ted/aa72305d473525c3179cdc471b3ef24a90c9a630_2880x1620.jpg?q=50&w=15", "duration": 978, "id": 1162, "ratings": [{"count": 291, "id": 10, "name": "Inspiring"}, {"count": 109, "id": 25, "name": "OK"}, {"count": 227, "id": 24, "name": "Persuasive"}, {"count": 177, "id": 8, "name": "Informative"}, {"count": 109, "id": 22, "name": "Fascinating"}, {"count": 34, "id": 2, "name": "Confusing"}, {"count": 123, "id": 21, "name": "Unconvincing"}, {"count": 102, "id": 11, "name": "Longwinded"}, {"count": 76, "id": 26, "name": "Obnoxious"}, {"count": 77, "id": 3, "name": "Courageous"}, {"count": 37, "id": 9, "name": "Ingenious"}, {"count": 19, "id": 23, "name": "Jaw-dropping"}, {"count": 26, "id": 1, "name": "Beautiful"}, {"count": 22, "id": 7, "name": "Funny"}], "speakers": [{"description": "Philosopher, entrepreneur", "firstname": "Damon", "title": "", "lastname": "Horowitz", "middleinitial": "", "whylisten": "<p>Damon Horowitz is a philosophy professor and serial entrepreneur. He recently joined Google as In-House Philosopher / Director of Engineering, heading development of several initiatives involving social and search. He came to Google from Aardvark, the social search engine, where he was co-founder and CTO, overseeing product development and research strategy. Prior to Aardvark, Horowitz built several companies around applications of intelligent language processing. He co-founded Perspecta (acquired by Excite), was lead architect for Novation Biosciences (acquired by Agilent), and co-founded NewsDB (now Daylife). </p><p>Horowitz teaches courses in philosophy, cognitive science, and computer science at several institutions, including Stanford, NYU, University of Pennsylvania and <a href=\\"http://www.prisonuniversityproject.org/\\" target=\\"_blank\\">San Quentin State Prison</a>. </p><p>Get more information on the <a href=\\"http://www.prisonuniversityproject.org/\\" target=\\"_blank\\">Prison University Project &gt;&gt;</a>  </p>", "slug": "damon_horowitz", "whotheyare": "Damon Horowitz explores what is possible at the boundaries of technology and the humanities.", "whatotherssay": "Damon seems to have realized in reverse order that the horse belongs in front of the carriage -- that humanity should lead technology, not vice versa.", "id": 1062, "photo_url": "https://pe.tedcdn.com/images/ted/266bebcd762dca63b6bc5cb255647c96fc475bf4_254x191.jpg"}], "title": "We need a \\"moral operating system\\"", "take_action": null, "comments": 4476, "more_resources": null, "hero": "https://pe.tedcdn.com/images/ted/aa72305d473525c3179cdc471b3ef24a90c9a630_2880x1620.jpg", "description": "Damon Horowitz reviews the enormous new powers that technology gives us: to know more -- and more about each other -- than ever before. Drawing the audience into a philosophical discussion, Horowitz invites us to pay new attention to the basic philosophy -- the ethical principles -- behind the burst of invention remaking our world. Where\'s the moral operating system that allows us to make sense of it?", "tags": ["TEDx", "culture", "philosophy", "technology"], "downloads": {"languages": [{"languageCode": "sq", "endonym": "Shqip", "isRtl": false, "ianaCode": "sq", "languageName": "Albanian"}, {"languageCode": "ar", "endonym": "\\u0627\\u0644\\u0639\\u0631\\u0628\\u064a\\u0629", "isRtl": true, "ianaCode": "ar", "languageName": "Arabic"}, {"languageCode": "bg", "endonym": "\\u0431\\u044a\\u043b\\u0433\\u0430\\u0440\\u0441\\u043a\\u0438", "isRtl": false, "ianaCode": "bg", "languageName": "Bulgarian"}, {"languageCode": "zh-cn", "endonym": "\\u4e2d\\u6587 (\\u7b80\\u4f53)", "isRtl": false, "ianaCode": "zh-Hans", "languageName": "Chinese, Simplified"}, {"languageCode": "zh-tw", "endonym": "\\u4e2d\\u6587 (\\u7e41\\u9ad4)", "isRtl": false, "ianaCode": "zh-Hant", "languageName": "Chinese, Traditional"}, {"languageCode": "hr", "endonym": "Hrvatski", "isRtl": false, "ianaCode": "hr", "languageName": "Croatian"}, {"languageCode": "cs", "endonym": "\\u010ce\\u0161tina", "isRtl": false, "ianaCode": "cs", "languageName": "Czech"}, {"languageCode": "nl", "endonym": "Nederlands", "isRtl": false, "ianaCode": "nl", "languageName": "Dutch"}, {"languageCode": "en", "endonym": "English", "isRtl": false, "ianaCode": "en", "languageName": "English"}, {"languageCode": "fr", "endonym": "Fran\\u00e7ais", "isRtl": false, "ianaCode": "fr", "languageName": "French"}, {"languageCode": "de", "endonym": "Deutsch", "isRtl": false, "ianaCode": "de", "languageName": "German"}, {"languageCode": "el", "endonym": "\\u0395\\u03bb\\u03bb\\u03b7\\u03bd\\u03b9\\u03ba\\u03ac", "isRtl": false, "ianaCode": "el", "languageName": "Greek"}, {"languageCode": "he", "endonym": "\\u05e2\\u05d1\\u05e8\\u05d9\\u05ea", "isRtl": true, "ianaCode": "he", "languageName": "Hebrew"}, {"languageCode": "hu", "endonym": "Magyar", "isRtl": false, "ianaCode": "hu", "languageName": "Hungarian"}, {"languageCode": "id", "endonym": "Bahasa Indonesia", "isRtl": false, "ianaCode": "id", "languageName": "Indonesian"}, {"languageCode": "it", "endonym": "Italiano", "isRtl": false, "ianaCode": "it", "languageName": "Italian"}, {"languageCode": "ja", "endonym": "\\u65e5\\u672c\\u8a9e", "isRtl": false, "ianaCode": "ja", "languageName": "Japanese"}, {"languageCode": "ko", "endonym": "\\ud55c\\uad6d\\uc5b4", "isRtl": false, "ianaCode": "ko", "languageName": "Korean"}, {"languageCode": "nb", "endonym": "Norsk bokm\\u00e5l", "isRtl": false, "ianaCode": "nb", "languageName": "Norwegian Bokmal"}, {"languageCode": "fa", "endonym": "\\u0641\\u0627\\u0631\\u0633\\u0649", "isRtl": true, "ianaCode": "fa", "languageName": "Persian"}, {"languageCode": "pl", "endonym": "Polski", "isRtl": false, "ianaCode": "pl", "languageName": "Polish"}, {"languageCode": "pt", "endonym": "Portugu\\u00eas de Portugal", "isRtl": false, "ianaCode": "pt", "languageName": "Portuguese"}, {"languageCode": "pt-br", "endonym": "Portugu\\u00eas brasileiro", "isRtl": false, "ianaCode": "pt-BR", "languageName": "Portuguese, Brazilian"}, {"languageCode": "ro", "endonym": "Rom\\u00e2n\\u0103", "isRtl": false, "ianaCode": "ro", "languageName": "Romanian"}, {"languageCode": "ru", "endonym": "\\u0420\\u0443\\u0441\\u0441\\u043a\\u0438\\u0439", "isRtl": false, "ianaCode": "ru", "languageName": "Russian"}, {"languageCode": "sr", "endonym": "\\u0421\\u0440\\u043f\\u0441\\u043a\\u0438, Srpski", "isRtl": false, "ianaCode": "sr", "languageName": "Serbian"}, {"languageCode": "es", "endonym": "Espa\\u00f1ol", "isRtl": false, "ianaCode": "es", "languageName": "Spanish"}, {"languageCode": "th", "endonym": "\\u0e20\\u0e32\\u0e29\\u0e32\\u0e44\\u0e17\\u0e22", "isRtl": false, "ianaCode": "th", "languageName": "Thai"}, {"languageCode": "tr", "endonym": "T\\u00fcrk\\u00e7e", "isRtl": false, "ianaCode": "tr", "languageName": "Turkish"}, {"languageCode": "vi", "endonym": "Ti\\u1ebfng Vi\\u1ec7t", "isRtl": false, "ianaCode": "vi", "languageName": "Vietnamese"}], "subtitledDownloads": {"el": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-el.mp4", "name": "Greek", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-el.mp4"}, "en": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-en.mp4", "name": "English", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-en.mp4"}, "vi": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-vi.mp4", "name": "Vietnamese", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-vi.mp4"}, "it": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-it.mp4", "name": "Italian", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-it.mp4"}, "ar": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-ar.mp4", "name": "Arabic", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-ar.mp4"}, "pt-br": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-pt-br.mp4", "name": "Portuguese, Brazilian", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-pt-br.mp4"}, "cs": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-cs.mp4", "name": "Czech", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-cs.mp4"}, "id": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-id.mp4", "name": "Indonesian", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-id.mp4"}, "es": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-es.mp4", "name": "Spanish", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-es.mp4"}, "ru": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-ru.mp4", "name": "Russian", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-ru.mp4"}, "nl": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-nl.mp4", "name": "Dutch", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-nl.mp4"}, "pt": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-pt.mp4", "name": "Portuguese", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-pt.mp4"}, "zh-tw": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-zh-tw.mp4", "name": "Chinese, Traditional", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-zh-tw.mp4"}, "nb": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-nb.mp4", "name": "Norwegian Bokmal", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-nb.mp4"}, "tr": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-tr.mp4", "name": "Turkish", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-tr.mp4"}, "zh-cn": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-zh-cn.mp4", "name": "Chinese, Simplified", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-zh-cn.mp4"}, "th": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-th.mp4", "name": "Thai", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-th.mp4"}, "ro": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-ro.mp4", "name": "Romanian", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-ro.mp4"}, "pl": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-pl.mp4", "name": "Polish", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-pl.mp4"}, "fr": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-fr.mp4", "name": "French", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-fr.mp4"}, "bg": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-bg.mp4", "name": "Bulgarian", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-bg.mp4"}, "hr": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-hr.mp4", "name": "Croatian", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-hr.mp4"}, "de": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-de.mp4", "name": "German", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-de.mp4"}, "hu": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-hu.mp4", "name": "Hungarian", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-hu.mp4"}, "fa": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-fa.mp4", "name": "Persian", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-fa.mp4"}, "ja": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-ja.mp4", "name": "Japanese", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-ja.mp4"}, "he": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-he.mp4", "name": "Hebrew", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-he.mp4"}, "sr": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-sr.mp4", "name": "Serbian", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-sr.mp4"}, "sq": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-sq.mp4", "name": "Albanian", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-sq.mp4"}, "ko": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p-ko.mp4", "name": "Korean", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-low-ko.mp4"}}, "nativeDownloads": {"high": "https://download.ted.com/talks/DamonHorowitz_2011X-480p.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "medium": "https://download.ted.com/talks/DamonHorowitz_2011X.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "low": "https://download.ted.com/talks/DamonHorowitz_2011X-light.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22"}, "id": 1162, "audioDownload": null}, "related_talks": [{"viewed_count": 768925, "hero": "https://pe.tedcdn.com/images/ted/c331eb763dfc01984cd1e0c42c6d8d24c5731578_1600x1200.jpg", "title": "A philosophical quest for our biggest problems", "id": 44, "speaker": "Nick Bostrom", "duration": 1012, "slug": "nick_bostrom_on_our_biggest_problems"}, {"viewed_count": 1157694, "hero": "https://pe.tedcdn.com/images/ted/3f6dda85c262a19435f481dfe30c3ebe469d874a_800x600.jpg", "title": "Philosophy in prison", "id": 1286, "speaker": "Damon Horowitz", "duration": 230, "slug": "damon_horowitz_philosophy_in_prison"}, {"viewed_count": 2936333, "hero": "https://pe.tedcdn.com/images/ted/8bd64f43647646929b8ab3ca8b150a9517a6e935_2400x1800.jpg", "title": "Why does the universe exist?", "id": 2072, "speaker": "Jim Holt", "duration": 1037, "slug": "jim_holt_why_does_the_universe_exist"}, {"viewed_count": 1018382, "hero": "https://pe.tedcdn.com/images/ted/d24aaaf281eb21e0df2060b9163361973b02cf77_1600x1200.jpg", "title": "\\"Mother of Pearl,\\" \\"If I Had You\\"", "id": 296, "speaker": "Nellie McKay", "duration": 334, "slug": "nellie_mckay_sings_feminists_and_if_i_had_you"}, {"viewed_count": 405122, "hero": "https://pe.tedcdn.com/images/ted/24710_480x360.jpg", "title": "\\"Tembererana\\"", "id": 188, "speaker": "Raul Midon", "duration": 640, "slug": "raul_midon_plays_all_the_answers_and_tembererana"}, {"viewed_count": 3458799, "hero": "https://pe.tedcdn.com/images/ted/9762ce97cf4f10777681dd357532a00300f7d4f2_2880x1620.jpg", "title": "Science can answer moral questions", "id": 801, "speaker": "Sam Harris", "duration": 1386, "slug": "sam_harris_science_can_show_what_s_right"}], "recorded_at": "2011-05-14T00:00:00.000+00:00", "slug": "damon_horowitz", "speaker_name": "Damon Horowitz", "viewed_count": 685794, "event_badge": null, "event_blurb": "This talk was presented to a local audience at TEDxSiliconValley, an independent event. TED editors featured it among our selections on the home page.", "recommendations": null, "corrections": null}], "event": "TEDxSiliconValley", "name": "Damon Horowitz: We need a \\"moral operating system\\""}'
p439
sS'keywords'
p440
(lp441
VTEDx
p442
aVculture
p443
aVphilosophy
p444
aVtechnology
p445
asS'datecrawled'
p446
g434
(S'\x07\xe1\n\x17\x02\x005\x02\xde\xc0'
tRp447
sS'id'
p448
I1162
ss.