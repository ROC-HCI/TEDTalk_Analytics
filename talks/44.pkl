(dp1
S'talk_transcript'
p2
(lp3
(lp4
VI want to talk today about \u2014
p5
aVI've been asked to take the long view, and I'm going to tell you what
p6
aVI think are the three biggest problems for humanity
p7
aVfrom this long point of view.
p8
aVSome of these have already been touched upon by other speakers,
p9
aVwhich is encouraging.
p10
aVIt seems that there's not just one person
p11
aVwho thinks that these problems are important.
p12
aa(lp13
VThe first is \u2014 death is a big problem.
p14
aVIf you look at the statistics,
p15
aVthe odds are not very favorable to us.
p16
aVSo far, most people who have lived have also died.
p17
aVRoughly 90 percent of everybody who has been alive has died by now.
p18
aVSo the annual death rate adds up to 150,000 \u2014
p19
aVsorry, the daily death rate \u2014 150,000 people per day,
p20
aVwhich is a huge number by any standard.
p21
aVThe annual death rate, then, becomes 56 million.
p22
aVIf we just look at the single, biggest cause of death \u2014 aging \u2014
p23
aVit accounts for roughly two-thirds of all human people who die.
p24
aVThat adds up to an annual death toll
p25
aVof greater than the population of Canada.
p26
aVSometimes, we don't see a problem
p27
aVbecause either it's too familiar or it's too big.
p28
aVCan't see it because it's too big.
p29
aVI think death might be both too familiar and too big
p30
aVfor most people to see it as a problem.
p31
aa(lp32
VOnce you think about it, you see this is not statistical points;
p33
aVthese are \u2014 let's see, how far have I talked?
p34
aVI've talked for three minutes.
p35
aVSo that would be, roughly, 324 people have died since I've begun speaking.
p36
aVPeople like \u2014 it's roughly the population in this room has just died.
p37
aVNow, the human cost of that is obvious,
p38
aVonce you start to think about it \u2014 the suffering, the loss \u2014
p39
aVit's also, economically, enormously wasteful.
p40
aVI just look at the information, and knowledge, and experience
p41
aVthat is lost due to natural causes of death in general,
p42
aVand aging, in particular.
p43
aa(lp44
VSuppose we approximated one person with one book?
p45
aVNow, of course, this is an underestimation.
p46
aVA person's lifetime of learning and experience
p47
aVis a lot more than you could put into a single book.
p48
aVBut let's suppose we did this.
p49
aV52 million people die of natural causes each year
p50
aVcorresponds, then, to 52 million volumes destroyed.
p51
aVLibrary of Congress holds 18 million volumes.
p52
aVWe are upset about the burning of the Library of Alexandria.
p53
aVIt's one of the great cultural tragedies
p54
aVthat we remember, even today.
p55
aVBut this is the equivalent of three Libraries of Congress \u2014
p56
aVburnt down, forever lost \u2014 each year.
p57
aa(lp58
VSo that's the first big problem.
p59
aVAnd I wish Godspeed to Aubrey de Grey,
p60
aVand other people like him,
p61
aVto try to do something about this as soon as possible.
p62
aVExistential risk \u2014 the second big problem.
p63
aVExistential risk is a threat to human survival, or to the long-term potential of our species.
p64
aVNow, why do I say that this is a big problem?
p65
aVWell, let's first look at the probability \u2014
p66
aVand this is very, very difficult to estimate \u2014
p67
aVbut there have been only four studies on this in recent years,
p68
aVwhich is surprising.
p69
aVYou would think that it would be of some interest
p70
aVto try to find out more about this given that the stakes are so big,
p71
aVbut it's a very neglected area.
p72
aa(lp73
VBut there have been four studies \u2014
p74
aVone by John Lesley, wrote a book on this.
p75
aVHe estimated a probability that we will fail
p76
aVto survive the current century: 50 percent.
p77
aVSimilarly, the Astronomer Royal, whom we heard speak yesterday,
p78
aValso has a 50 percent probability estimate.
p79
aVAnother author doesn't give any numerical estimate,
p80
aVbut says the probability is significant that it will fail.
p81
aVI wrote a long paper on this.
p82
aVI said assigning a less than 20 percent probability would be a mistake
p83
aVin light of the current evidence we have.
p84
aVNow, the exact figures here,
p85
aVwe should take with a big grain of salt,
p86
aVbut there seems to be a consensus that the risk is substantial.
p87
aVEverybody who has looked at this and studied it agrees.
p88
aa(lp89
VNow, if we think about what just reducing
p90
aVthe probability of human extinction by just one percentage point \u2014
p91
aVnot very much \u2014 so that's equivalent to 60 million lives saved,
p92
aVif we just count the currently living people, the current generation.
p93
aVNow one percent of six billion people is equivalent to 60 million.
p94
aVSo that's a large number.
p95
aVIf we were to take into account future generations
p96
aVthat will never come into existence if we blow ourselves up,
p97
aVthen the figure becomes astronomical.
p98
aVIf we could eventually colonize a chunk of the universe \u2014
p99
aVthe Virgo supercluster \u2014
p100
aVmaybe it will take us 100 million years to get there,
p101
aVbut if we go extinct we never will.
p102
aVThen, even a one percentage point reduction
p103
aVin the extinction risk could be equivalent
p104
aVto this astronomical number \u2014 10 to the power of 32.
p105
aa(lp106
VSo if you take into account future generations as much as our own,
p107
aVevery other moral imperative of philanthropic cost just becomes irrelevant.
p108
aVThe only thing you should focus on
p109
aVwould be to reduce existential risk
p110
aVbecause even the tiniest decrease in existential risk
p111
aVwould just overwhelm any other benefit you could hope to achieve.
p112
aVAnd even if you just look at the current people,
p113
aVand ignore the potential that would be lost if we went extinct,
p114
aVit should still have a high priority.
p115
aVNow, let me spend the rest of my time on the third big problem,
p116
aVbecause it's more subtle and perhaps difficult to grasp.
p117
aVThink about some time in your life \u2014
p118
aVsome people might never have experienced it \u2014 but some people,
p119
aVthere are just those moments that you have experienced
p120
aVwhere life was fantastic.
p121
aa(lp122
VIt might have been at the moment of some great, creative inspiration
p123
aVyou might have had when you just entered this flow stage.
p124
aVOr when you understood something you had never done before.
p125
aVOr perhaps in the ecstasy of romantic love.
p126
aVOr an aesthetic experience \u2014 a sunset or a great piece of art.
p127
aVEvery once in a while we have these moments,
p128
aVand we realize just how good life can be when it's at its best.
p129
aVAnd you wonder, why can't it be like that all the time?
p130
aVYou just want to cling onto this.
p131
aVAnd then, of course, it drifts back into ordinary life and the memory fades.
p132
aVAnd it's really difficult to recall, in a normal frame of mind,
p133
aVjust how good life can be at its best.
p134
aVOr how bad it can be at its worst.
p135
aa(lp136
VThe third big problem is that life isn't usually
p137
aVas wonderful as it could be.
p138
aVI think that's a big, big problem.
p139
aVIt's easy to say what we don't want.
p140
aVHere are a number of things that we don't want \u2014
p141
aVillness, involuntary death, unnecessary suffering, cruelty,
p142
aVstunted growth, memory loss, ignorance, absence of creativity.
p143
aVSuppose we fixed these things \u2014 we did something about all of these.
p144
aVWe were very successful.
p145
aVWe got rid of all of these things.
p146
aVWe might end up with something like this,
p147
aVwhich is \u2014 I mean, it's a heck of a lot better than that.
p148
aVBut is this really the best we can dream of?
p149
aVIs this the best we can do?
p150
aa(lp151
VOr is it possible to find something a little bit more inspiring to work towards?
p152
aVAnd if we think about this,
p153
aVI think it's very clear that there are ways
p154
aVin which we could change things, not just by eliminating negatives,
p155
aVbut adding positives.
p156
aVOn my wish list, at least, would be:
p157
aVmuch longer, healthier lives, greater subjective well-being,
p158
aVenhanced cognitive capacities, more knowledge and understanding,
p159
aVunlimited opportunity for personal growth
p160
aVbeyond our current biological limits, better relationships,
p161
aVan unbounded potential for spiritual, moral
p162
aVand intellectual development.
p163
aa(lp164
VIf we want to achieve this, what, in the world, would have to change?
p165
aVAnd this is the answer \u2014 we would have to change.
p166
aVNot just the world around us, but we, ourselves.
p167
aVNot just the way we think about the world, but the way we are \u2014 our very biology.
p168
aVHuman nature would have to change.
p169
aVNow, when we think about changing human nature,
p170
aVthe first thing that comes to mind
p171
aVare these human modification technologies \u2014
p172
aVgrowth hormone therapy, cosmetic surgery,
p173
aVstimulants like Ritalin, Adderall, anti-depressants,
p174
aVanabolic steroids, artificial hearts.
p175
aVIt's a pretty pathetic list.
p176
aVThey do great things for a few people
p177
aVwho suffer from some specific condition,
p178
aVbut for most people, they don't really transform
p179
aVwhat it is to be human.
p180
aVAnd they also all seem a little bit \u2014
p181
aVmost people have this instinct that, well, sure,
p182
aVthere needs to be anti-depressants for the really depressed people.
p183
aVBut there's a kind of queasiness
p184
aVthat these are unnatural in some way.
p185
aa(lp186
VIt's worth recalling that there are a lot of other
p187
aVmodification technologies and enhancement technologies that we use.
p188
aVWe have skin enhancements, clothing.
p189
aVAs far as I can see, all of you are users of this
p190
aVenhancement technology in this room, so that's a great thing.
p191
aVMood modifiers have been used from time immemorial \u2014
p192
aVcaffeine, alcohol, nicotine, immune system enhancement,
p193
aVvision enhancement, anesthetics \u2014
p194
aVwe take that very much for granted,
p195
aVbut just think about how great progress that is \u2014
p196
aVlike, having an operation before anesthetics was not fun.
p197
aVContraceptives, cosmetics and brain reprogramming techniques \u2014
p198
aVthat sounds ominous,
p199
aa(lp200
Vbut the distinction between what is a technology \u2014
p201
aVa gadget would be the archetype \u2014
p202
aVand other ways of changing and rewriting human nature is quite subtle.
p203
aVSo if you think about what it means to learn arithmetic or to learn to read,
p204
aVyou're actually, literally rewriting your own brain.
p205
aVYou're changing the microstructure of your brain as you go along.
p206
aVSo in a broad sense, we don't need to think about technology
p207
aVas only little gadgets, like these things here,
p208
aVbut even institutions and techniques,
p209
aVpsychological methods and so forth.
p210
aVForms of organization can have a profound impact on human nature.
p211
aa(lp212
VLooking ahead, there is a range of technologies
p213
aVthat are almost certain to be developed sooner or later.
p214
aVWe are very ignorant about what the time scale for these things are,
p215
aVbut they all are consistent with everything we know
p216
aVabout physical laws, laws of chemistry, etc.
p217
aVIt's possible to assume,
p218
aVsetting aside a possibility of catastrophe,
p219
aVthat sooner or later we will develop all of these.
p220
aVAnd even just a couple of these would be enough
p221
aVto transform the human condition.
p222
aa(lp223
VSo let's look at some of the dimensions of human nature
p224
aVthat seem to leave room for improvement.
p225
aVHealth span is a big and urgent thing,
p226
aVbecause if you're not alive,
p227
aVthen all the other things will be to little avail.
p228
aVIntellectual capacity \u2014 let's take that box,
p229
aVwhich falls into a lot of different sub-categories:
p230
aVmemory, concentration, mental energy, intelligence, empathy.
p231
aVThese are really great things.
p232
aVPart of the reason why we value these traits
p233
aVis that they make us better at competing with other people \u2014
p234
aVthey're positional goods.
p235
aVBut part of the reason \u2014
p236
aVand that's the reason why we have ethical ground for pursuing these \u2014
p237
aVis that they're also intrinsically valuable.
p238
aVIt's just better to be able to understand more of the world around you
p239
aVand the people that you are communicating with,
p240
aVand to remember what you have learned.
p241
aVModalities and special faculties.
p242
aVNow, the human mind is not a single unitary information processor,
p243
aVbut it has a lot of different, special, evolved modules
p244
aVthat do specific things for us.
p245
aVIf you think about what we normally take as giving life a lot of its meaning \u2014
p246
aVmusic, humor, eroticism, spirituality, aesthetics,
p247
aVnurturing and caring, gossip, chatting with people \u2014
p248
aa(lp249
Vall of these, very likely, are enabled by a special circuitry
p250
aVthat we humans have,
p251
aVbut that you could have another intelligent life form that lacks these.
p252
aVWe're just lucky that we have the requisite neural machinery
p253
aVto process music and to appreciate it and enjoy it.
p254
aVAll of these would enable, in principle \u2014 be amenable to enhancement.
p255
aVSome people have a better musical ability
p256
aVand ability to appreciate music than others have.
p257
aVIt's also interesting to think about what other things are \u2014
p258
aVso if these all enabled great values,
p259
aVwhy should we think that evolution has happened to provide us
p260
aVwith all the modalities we would need to engage
p261
aVwith other values that there might be?
p262
aa(lp263
VImagine a species
p264
aVthat just didn't have this neural machinery for processing music.
p265
aVAnd they would just stare at us with bafflement
p266
aVwhen we spend time listening to a beautiful performance,
p267
aVlike the one we just heard \u2014 because of people making stupid movements,
p268
aVand they would be really irritated and wouldn't see what we were up to.
p269
aVBut maybe they have another faculty, something else
p270
aVthat would seem equally irrational to us,
p271
aVbut they actually tap into some great possible value there.
p272
aVBut we are just literally deaf to that kind of value.
p273
aVSo we could think of adding on different,
p274
aVnew sensory capacities and mental faculties.
p275
aVBodily functionality and morphology and affective self-control.
p276
aVGreater subjective well-being.
p277
aVBe able to switch between relaxation and activity \u2014
p278
aVbeing able to go slow when you need to do that, and to speed up.
p279
aVAble to switch back and forth more easily
p280
aVwould be a neat thing to be able to do \u2014
p281
aVeasier to achieve the flow state,
p282
aVwhen you're totally immersed in something you are doing.
p283
aVConscientiousness and sympathy.
p284
aVThe ability to \u2014 it's another interesting application
p285
aVthat would have large social ramification, perhaps.
p286
aVIf you could actually choose to preserve your romantic attachments to one person,
p287
aVundiminished through time,
p288
aVso that wouldn't have to \u2014 love would never have to fade if you didn't want it to.
p289
aVThat's probably not all that difficult.
p290
aVIt might just be a simple hormone or something that could do this.
p291
aa(lp292
VIt's been done in voles.
p293
aVYou can engineer a prairie vole to become monogamous
p294
aVwhen it's naturally polygamous.
p295
aVIt's just a single gene.
p296
aVMight be more complicated in humans, but perhaps not that much.
p297
aVThis is the last picture that I want to \u2014
p298
aVnow we've got to use the laser pointer.
p299
aVA possible mode of being here would be a way of life \u2014
p300
aVa way of being, experiencing, thinking, seeing,
p301
aVinteracting with the world.
p302
aVDown here in this little corner, here, we have the little sub-space
p303
aVof this larger space that is accessible to human beings \u2014
p304
aVbeings with our biological capacities.
p305
aVIt's a part of the space that's accessible to animals;
p306
aVsince we are animals, we are a subset of that.
p307
aa(lp308
VAnd then you can imagine some enhancements of human capacities.
p309
aVThere would be different modes of being you could experience
p310
aVif you were able to stay alive for, say, 200 years.
p311
aVThen you could live sorts of lives and accumulate wisdoms
p312
aVthat are just not possible for humans as we currently are.
p313
aVSo then, you move off to this larger sphere of "human +,"
p314
aVand you could continue that process and eventually
p315
aVexplore a lot of this larger space of possible modes of being.
p316
aa(lp317
VNow, why is that a good thing to do?
p318
aVWell, we know already that in this little human circle there,
p319
aVthere are these enormously wonderful and worthwhile modes of being \u2014
p320
aVhuman life at its best is wonderful.
p321
aVWe have no reason to believe that within this much, much larger space
p322
aVthere would not also be extremely worthwhile modes of being,
p323
aVperhaps ones that would be way beyond our wildest ability
p324
aVeven to imagine or dream about.
p325
aVAnd so, to fix this third problem,
p326
aVI think we need \u2014 slowly, carefully, with ethical wisdom and constraint \u2014
p327
aVdevelop the means that enable us to go out in this larger space and explore it
p328
aVand find the great values that might hide there.
p329
aVThanks.
p330
aasS'transcript_micsec'
p331
(lp332
I11000
aI36000
aI100000
aI135000
aI178000
aI222000
aI265000
aI318000
aI371000
aI418000
aI463000
aI502000
aI565000
aI611000
aI648000
aI676000
aI756000
aI794000
aI884000
aI930000
aI958000
asS'talk_meta'
p333
(dp334
S'ratings'
p335
(dp336
S'ingenious'
p337
I78
sS'funny'
p338
I15
sS'inspiring'
p339
I108
sS'ok'
p340
I108
sS'fascinating'
p341
I119
sS'total_count'
p342
I1331
sS'persuasive'
p343
I47
sS'longwinded'
p344
I173
sS'informative'
p345
I151
sS'beautiful'
p346
I20
sS'jaw-dropping'
p347
I19
sS'obnoxious'
p348
I66
sS'confusing'
p349
I88
sS'courageous'
p350
I66
sS'unconvincing'
p351
I273
ssS'author'
p352
VNick_Bostrom;
p353
sS'url'
p354
S'https://www.ted.com/talks/nick_bostrom_on_our_biggest_problems'
p355
sS'vidlen'
p356
I1012
sS'totalviews'
p357
I768935
sS'title'
p358
VA philosophical quest for our biggest problems
p359
sS'downloadlink'
p360
Vhttps://download.ted.com/talks/NickBostrom_2005G.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22
p361
sS'datepublished'
p362
cdatetime
datetime
p363
(S'\x07\xd7\x04\x04\x14\x0b\x00\x00\x00\x00'
tRp364
sS'datefilmed'
p365
g363
(S'\x07\xd7\x04\x04\x14\x0b\x00\x00\x00\x00'
tRp366
sS'alldata_JSON'
p367
S'{"viewed_count": 768935, "speakers": [{"description": "Philosopher", "firstname": "Nick", "title": "", "lastname": "Bostrom", "middleinitial": "", "whylisten": "<p>Philosopher Nick Bostrom envisioned a future full of human enhancement, nanotechnology and machine intelligence long before they became mainstream concerns. From his famous simulation argument -- which identified some striking implications of rejecting the Matrix-like idea that humans are living in a computer simulation -- to his work on existential risk, Bostrom approaches both the inevitable and the speculative using the tools of philosophy, probability theory, and scientific analysis.<br /><br />Since 2005, Bostrom has led the <a href=\\"http://www.fhi.ox.ac.uk/\\" target=\\"_blank\\">Future of Humanity Institute</a>, a research group of mathematicians, philosophers and scientists at Oxford University tasked with investigating the big picture for the human condition and its future. He has been referred to as one of the most important thinkers of our age.</p><p>Nick was honored as one of <em>Foreign Policy</em>&#39;s 2015&nbsp;<a href=\\"http://2015globalthinkers.foreignpolicy.com/?utm_source=Sailthru&amp;utm_medium=email&amp;utm_campaign=New%20Campaign&amp;utm_term=Flashpoints#!advocates/detail/bostrom\\" target=\\"_blank\\">Global Thinkers</a> .</p><p>His recent book <a href=\\"http://geni.us/superintelligence\\" target=\\"_blank\\"><em>Superintelligence</em></a>  advances the ominous idea that &ldquo;the first ultraintelligent machine is the last invention that man need ever make.&rdquo;</p>", "slug": "nick_bostrom", "whotheyare": "Nick Bostrom asks big questions: What should we do, as individuals and as a species, to optimize our long-term prospects? Will humanity\\u2019s technological advancements ultimately destroy us?", "whatotherssay": "Bostrom cogently argues that the prospect of superintelligent machines is \\u2018the most important and most daunting challenge humanity has ever faced.\\u2019 If we fail to meet this challenge, he concludes, malevolent or indifferent artificial intelligence (AI) will likely destroy us all.", "id": 39, "photo_url": "https://pe.tedcdn.com/images/ted/56aef93b63301ffe15f201c8b357ee2161abf955_254x191.jpg"}], "current_talk": 44, "description": "Oxford philosopher and transhumanist Nick Bostrom examines the future of humankind and asks whether we might alter the fundamental nature of humanity to solve our most intrinsic problems.", "language": "en", "url": "https://www.ted.com/talks/nick_bostrom_on_our_biggest_problems", "media": {"internal": {"podcast-high-en": {"uri": "https://download.ted.com/talks/NickBostrom_2005G-480p-en.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 118074814}, "podcast-low-en": {"uri": "https://download.ted.com/talks/NickBostrom_2005G-low-en.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 10120339}, "podcast-high": {"uri": "https://download.ted.com/talks/NickBostrom_2005G-480p.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 118075041}, "180k": {"uri": "https://download.ted.com/talks/NickBostrom_2005G-180k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 23175818}, "64k": {"uri": "https://download.ted.com/talks/NickBostrom_2005G-64k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 8345148}, "1500k": {"uri": "https://download.ted.com/talks/NickBostrom_2005G-1500k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 183961128}, "450k": {"uri": "https://download.ted.com/talks/NickBostrom_2005G-450k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 57501079}, "podcast-regular": {"uri": "https://download.ted.com/talks/NickBostrom_2005G.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 57558437}, "950k": {"uri": "https://download.ted.com/talks/NickBostrom_2005G-950k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 117968351}, "podcast-light": {"uri": "https://download.ted.com/talks/NickBostrom_2005G-light.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 8441646}, "320k": {"uri": "https://download.ted.com/talks/NickBostrom_2005G-320k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 40995961}, "600k": {"uri": "https://download.ted.com/talks/NickBostrom_2005G-600k.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "mime_type": "video/mp4", "filesize_bytes": 76407918}}}, "comments": {"count": 131, "id": 43, "talk_id": 44}, "slug": "nick_bostrom_on_our_biggest_problems", "threadId": 43, "talks": [{"event": "TEDGlobal 2005", "player_talks": [{"event": "TEDGlobal 2005", "slug": "nick_bostrom_on_our_biggest_problems", "filmed": 1121299200, "targeting": {"event": "TEDGlobal 2005", "tag": "biotech,culture,future,global issues,happiness,philosophy,technology", "id": 44, "talk": "nick_bostrom_on_our_biggest_problems", "year": "2005"}, "adDuration": "3.33", "external": null, "title": "A philosophical quest for our biggest problems", "postAdDuration": "0.83", "published": 1175731860, "thumb": "https://pi.tedcdn.com/r/pe.tedcdn.com/images/ted/c331eb763dfc01984cd1e0c42c6d8d24c5731578_1600x1200.jpg?quality=89&w=600", "name": "Nick Bostrom: A philosophical quest for our biggest problems", "languages": [{"languageCode": "ar", "endonym": "\\u0627\\u0644\\u0639\\u0631\\u0628\\u064a\\u0629", "isRtl": true, "ianaCode": "ar", "languageName": "Arabic"}, {"languageCode": "bg", "endonym": "\\u0431\\u044a\\u043b\\u0433\\u0430\\u0440\\u0441\\u043a\\u0438", "isRtl": false, "ianaCode": "bg", "languageName": "Bulgarian"}, {"languageCode": "zh-cn", "endonym": "\\u4e2d\\u6587 (\\u7b80\\u4f53)", "isRtl": false, "ianaCode": "zh-Hans", "languageName": "Chinese, Simplified"}, {"languageCode": "zh-tw", "endonym": "\\u4e2d\\u6587 (\\u7e41\\u9ad4)", "isRtl": false, "ianaCode": "zh-Hant", "languageName": "Chinese, Traditional"}, {"languageCode": "hr", "endonym": "Hrvatski", "isRtl": false, "ianaCode": "hr", "languageName": "Croatian"}, {"languageCode": "cs", "endonym": "\\u010ce\\u0161tina", "isRtl": false, "ianaCode": "cs", "languageName": "Czech"}, {"languageCode": "en", "endonym": "English", "isRtl": false, "ianaCode": "en", "languageName": "English"}, {"languageCode": "fr", "endonym": "Fran\\u00e7ais", "isRtl": false, "ianaCode": "fr", "languageName": "French"}, {"languageCode": "ka", "endonym": "\\u10e5\\u10d0\\u10e0\\u10d7\\u10e3\\u10da\\u10d8", "isRtl": false, "ianaCode": "ka", "languageName": "Georgian"}, {"languageCode": "de", "endonym": "Deutsch", "isRtl": false, "ianaCode": "de", "languageName": "German"}, {"languageCode": "he", "endonym": "\\u05e2\\u05d1\\u05e8\\u05d9\\u05ea", "isRtl": true, "ianaCode": "he", "languageName": "Hebrew"}, {"languageCode": "hu", "endonym": "Magyar", "isRtl": false, "ianaCode": "hu", "languageName": "Hungarian"}, {"languageCode": "it", "endonym": "Italiano", "isRtl": false, "ianaCode": "it", "languageName": "Italian"}, {"languageCode": "ja", "endonym": "\\u65e5\\u672c\\u8a9e", "isRtl": false, "ianaCode": "ja", "languageName": "Japanese"}, {"languageCode": "ko", "endonym": "\\ud55c\\uad6d\\uc5b4", "isRtl": false, "ianaCode": "ko", "languageName": "Korean"}, {"languageCode": "mk", "endonym": "\\u043c\\u0430\\u043a\\u0435\\u0434\\u043e\\u043d\\u0441\\u043a\\u0438", "isRtl": false, "ianaCode": "mk", "languageName": "Macedonian"}, {"languageCode": "fa", "endonym": "\\u0641\\u0627\\u0631\\u0633\\u0649", "isRtl": true, "ianaCode": "fa", "languageName": "Persian"}, {"languageCode": "pl", "endonym": "Polski", "isRtl": false, "ianaCode": "pl", "languageName": "Polish"}, {"languageCode": "pt-br", "endonym": "Portugu\\u00eas brasileiro", "isRtl": false, "ianaCode": "pt-BR", "languageName": "Portuguese, Brazilian"}, {"languageCode": "ro", "endonym": "Rom\\u00e2n\\u0103", "isRtl": false, "ianaCode": "ro", "languageName": "Romanian"}, {"languageCode": "ru", "endonym": "\\u0420\\u0443\\u0441\\u0441\\u043a\\u0438\\u0439", "isRtl": false, "ianaCode": "ru", "languageName": "Russian"}, {"languageCode": "so", "endonym": "Af-Soomaali", "isRtl": false, "ianaCode": "so", "languageName": "Somali"}, {"languageCode": "es", "endonym": "Espa\\u00f1ol", "isRtl": false, "ianaCode": "es", "languageName": "Spanish"}, {"languageCode": "tr", "endonym": "T\\u00fcrk\\u00e7e", "isRtl": false, "ianaCode": "tr", "languageName": "Turkish"}], "nativeLanguage": "en", "tags": ["biotech", "culture", "future", "global issues", "happiness", "philosophy", "technology"], "speaker": "Nick Bostrom", "isSubtitleRequired": false, "introDuration": 11.82, "duration": 1012, "id": 44, "resources": {"h264": [{"bitrate": 320, "file": "https://download.ted.com/talks/NickBostrom_2005G-320k.mp4?dnt"}], "hls": {"maiTargeting": {"event": "TEDGlobal 2005", "tag": "biotech,culture,future,global issues,happiness,philosophy,technology", "id": 44, "talk": "nick_bostrom_on_our_biggest_problems", "year": "2005"}, "metadata": "https://hls.ted.com/talks/44.json", "stream": "https://hls.ted.com/talks/44.m3u8", "adUrl": "https://pubads.g.doubleclick.net/gampad/ads?ciu_szs=300x250%2C512x288%2C120x60%2C320x50%2C6x7%2C6x8&correlator=%5Bcorrelator%5D&cust_params=event%3DTEDGlobal%2B2005%26id%3D44%26tag%3Dbiotech%2Cculture%2Cfuture%2Cglobal%2Bissues%2Chappiness%2Cphilosophy%2Ctechnology%26talk%3Dnick_bostrom_on_our_biggest_problems%26year%3D2005&env=vp&gdfp_req=1&impl=s&iu=%2F5641%2Fmobile%2Fios%2Fweb&output=xml_vast2&sz=640x360&unviewed_position_start=1&url=%5Breferrer%5D"}}, "canonical": "https://www.ted.com/talks/nick_bostrom_on_our_biggest_problems"}], "hero_load": "https://pi.tedcdn.com/r/pe.tedcdn.com/images/ted/c331eb763dfc01984cd1e0c42c6d8d24c5731578_1600x1200.jpg?q=50&w=15", "duration": 1012, "id": 44, "ratings": [{"count": 78, "id": 9, "name": "Ingenious"}, {"count": 66, "id": 3, "name": "Courageous"}, {"count": 173, "id": 11, "name": "Longwinded"}, {"count": 273, "id": 21, "name": "Unconvincing"}, {"count": 88, "id": 2, "name": "Confusing"}, {"count": 151, "id": 8, "name": "Informative"}, {"count": 108, "id": 10, "name": "Inspiring"}, {"count": 119, "id": 22, "name": "Fascinating"}, {"count": 15, "id": 7, "name": "Funny"}, {"count": 20, "id": 1, "name": "Beautiful"}, {"count": 108, "id": 25, "name": "OK"}, {"count": 47, "id": 24, "name": "Persuasive"}, {"count": 66, "id": 26, "name": "Obnoxious"}, {"count": 19, "id": 23, "name": "Jaw-dropping"}], "speakers": [{"description": "Philosopher", "firstname": "Nick", "title": "", "lastname": "Bostrom", "middleinitial": "", "whylisten": "<p>Philosopher Nick Bostrom envisioned a future full of human enhancement, nanotechnology and machine intelligence long before they became mainstream concerns. From his famous simulation argument -- which identified some striking implications of rejecting the Matrix-like idea that humans are living in a computer simulation -- to his work on existential risk, Bostrom approaches both the inevitable and the speculative using the tools of philosophy, probability theory, and scientific analysis.<br /><br />Since 2005, Bostrom has led the <a href=\\"http://www.fhi.ox.ac.uk/\\" target=\\"_blank\\">Future of Humanity Institute</a>, a research group of mathematicians, philosophers and scientists at Oxford University tasked with investigating the big picture for the human condition and its future. He has been referred to as one of the most important thinkers of our age.</p><p>Nick was honored as one of <em>Foreign Policy</em>&#39;s 2015&nbsp;<a href=\\"http://2015globalthinkers.foreignpolicy.com/?utm_source=Sailthru&amp;utm_medium=email&amp;utm_campaign=New%20Campaign&amp;utm_term=Flashpoints#!advocates/detail/bostrom\\" target=\\"_blank\\">Global Thinkers</a> .</p><p>His recent book <a href=\\"http://geni.us/superintelligence\\" target=\\"_blank\\"><em>Superintelligence</em></a>  advances the ominous idea that &ldquo;the first ultraintelligent machine is the last invention that man need ever make.&rdquo;</p>", "slug": "nick_bostrom", "whotheyare": "Nick Bostrom asks big questions: What should we do, as individuals and as a species, to optimize our long-term prospects? Will humanity\\u2019s technological advancements ultimately destroy us?", "whatotherssay": "Bostrom cogently argues that the prospect of superintelligent machines is \\u2018the most important and most daunting challenge humanity has ever faced.\\u2019 If we fail to meet this challenge, he concludes, malevolent or indifferent artificial intelligence (AI) will likely destroy us all.", "id": 39, "photo_url": "https://pe.tedcdn.com/images/ted/56aef93b63301ffe15f201c8b357ee2161abf955_254x191.jpg"}], "title": "A philosophical quest for our biggest problems", "take_action": [], "comments": 43, "more_resources": [{"status": "approved", "publisher": "Oxford University Press", "start_at": null, "author": "Nick Bostrom", "headline": "*Superintelligence*", "link_url": "http://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111/ref=as_li_tf_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=0520271440&linkCode=as2&tag=teco06-20", "published": true, "eyebrow": null, "end_at": null, "image_url": "http://images.ted.com/images/ted/c44ab0b20ae06e3f65bf9146baf1414995c4fc2b_450x685.jpg", "year": "2014", "type": "book", "blurb": null}, {"status": "approved", "start_at": null, "headline": "Discover what you can learn from dragon-tyrants", "link_url": "http://www.nickbostrom.com/fable/dragon.html", "eyebrow": null, "end_at": null, "image_url": "", "published": true, "visible_url": "", "type": "external_website", "blurb": "Over at his site, read a fable by Nick Bostrom that gives playful insight into how to improve the quality and longitude of one\'s life."}], "hero": "https://pe.tedcdn.com/images/ted/c331eb763dfc01984cd1e0c42c6d8d24c5731578_1600x1200.jpg", "description": "Oxford philosopher and transhumanist Nick Bostrom examines the future of humankind and asks whether we might alter the fundamental nature of humanity to solve our most intrinsic problems.", "tags": ["biotech", "culture", "future", "global issues", "happiness", "philosophy", "technology"], "downloads": {"languages": [{"languageCode": "ar", "endonym": "\\u0627\\u0644\\u0639\\u0631\\u0628\\u064a\\u0629", "isRtl": true, "ianaCode": "ar", "languageName": "Arabic"}, {"languageCode": "bg", "endonym": "\\u0431\\u044a\\u043b\\u0433\\u0430\\u0440\\u0441\\u043a\\u0438", "isRtl": false, "ianaCode": "bg", "languageName": "Bulgarian"}, {"languageCode": "zh-cn", "endonym": "\\u4e2d\\u6587 (\\u7b80\\u4f53)", "isRtl": false, "ianaCode": "zh-Hans", "languageName": "Chinese, Simplified"}, {"languageCode": "zh-tw", "endonym": "\\u4e2d\\u6587 (\\u7e41\\u9ad4)", "isRtl": false, "ianaCode": "zh-Hant", "languageName": "Chinese, Traditional"}, {"languageCode": "hr", "endonym": "Hrvatski", "isRtl": false, "ianaCode": "hr", "languageName": "Croatian"}, {"languageCode": "cs", "endonym": "\\u010ce\\u0161tina", "isRtl": false, "ianaCode": "cs", "languageName": "Czech"}, {"languageCode": "en", "endonym": "English", "isRtl": false, "ianaCode": "en", "languageName": "English"}, {"languageCode": "fr", "endonym": "Fran\\u00e7ais", "isRtl": false, "ianaCode": "fr", "languageName": "French"}, {"languageCode": "ka", "endonym": "\\u10e5\\u10d0\\u10e0\\u10d7\\u10e3\\u10da\\u10d8", "isRtl": false, "ianaCode": "ka", "languageName": "Georgian"}, {"languageCode": "de", "endonym": "Deutsch", "isRtl": false, "ianaCode": "de", "languageName": "German"}, {"languageCode": "he", "endonym": "\\u05e2\\u05d1\\u05e8\\u05d9\\u05ea", "isRtl": true, "ianaCode": "he", "languageName": "Hebrew"}, {"languageCode": "hu", "endonym": "Magyar", "isRtl": false, "ianaCode": "hu", "languageName": "Hungarian"}, {"languageCode": "it", "endonym": "Italiano", "isRtl": false, "ianaCode": "it", "languageName": "Italian"}, {"languageCode": "ja", "endonym": "\\u65e5\\u672c\\u8a9e", "isRtl": false, "ianaCode": "ja", "languageName": "Japanese"}, {"languageCode": "ko", "endonym": "\\ud55c\\uad6d\\uc5b4", "isRtl": false, "ianaCode": "ko", "languageName": "Korean"}, {"languageCode": "mk", "endonym": "\\u043c\\u0430\\u043a\\u0435\\u0434\\u043e\\u043d\\u0441\\u043a\\u0438", "isRtl": false, "ianaCode": "mk", "languageName": "Macedonian"}, {"languageCode": "fa", "endonym": "\\u0641\\u0627\\u0631\\u0633\\u0649", "isRtl": true, "ianaCode": "fa", "languageName": "Persian"}, {"languageCode": "pl", "endonym": "Polski", "isRtl": false, "ianaCode": "pl", "languageName": "Polish"}, {"languageCode": "pt-br", "endonym": "Portugu\\u00eas brasileiro", "isRtl": false, "ianaCode": "pt-BR", "languageName": "Portuguese, Brazilian"}, {"languageCode": "ro", "endonym": "Rom\\u00e2n\\u0103", "isRtl": false, "ianaCode": "ro", "languageName": "Romanian"}, {"languageCode": "ru", "endonym": "\\u0420\\u0443\\u0441\\u0441\\u043a\\u0438\\u0439", "isRtl": false, "ianaCode": "ru", "languageName": "Russian"}, {"languageCode": "so", "endonym": "Af-Soomaali", "isRtl": false, "ianaCode": "so", "languageName": "Somali"}, {"languageCode": "es", "endonym": "Espa\\u00f1ol", "isRtl": false, "ianaCode": "es", "languageName": "Spanish"}, {"languageCode": "tr", "endonym": "T\\u00fcrk\\u00e7e", "isRtl": false, "ianaCode": "tr", "languageName": "Turkish"}], "subtitledDownloads": {"en": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-en.mp4", "name": "English", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-en.mp4"}, "it": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-it.mp4", "name": "Italian", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-it.mp4"}, "ar": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-ar.mp4", "name": "Arabic", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-ar.mp4"}, "pt-br": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-pt-br.mp4", "name": "Portuguese, Brazilian", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-pt-br.mp4"}, "cs": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-cs.mp4", "name": "Czech", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-cs.mp4"}, "es": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-es.mp4", "name": "Spanish", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-es.mp4"}, "ru": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-ru.mp4", "name": "Russian", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-ru.mp4"}, "zh-tw": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-zh-tw.mp4", "name": "Chinese, Traditional", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-zh-tw.mp4"}, "tr": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-tr.mp4", "name": "Turkish", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-tr.mp4"}, "zh-cn": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-zh-cn.mp4", "name": "Chinese, Simplified", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-zh-cn.mp4"}, "ro": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-ro.mp4", "name": "Romanian", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-ro.mp4"}, "pl": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-pl.mp4", "name": "Polish", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-pl.mp4"}, "fr": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-fr.mp4", "name": "French", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-fr.mp4"}, "bg": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-bg.mp4", "name": "Bulgarian", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-bg.mp4"}, "hr": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-hr.mp4", "name": "Croatian", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-hr.mp4"}, "de": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-de.mp4", "name": "German", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-de.mp4"}, "fa": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-fa.mp4", "name": "Persian", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-fa.mp4"}, "ja": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-ja.mp4", "name": "Japanese", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-ja.mp4"}, "he": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-he.mp4", "name": "Hebrew", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-he.mp4"}, "ka": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-ka.mp4", "name": "Georgian", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-ka.mp4"}, "ko": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-ko.mp4", "name": "Korean", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-ko.mp4"}, "mk": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-mk.mp4", "name": "Macedonian", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-mk.mp4"}, "so": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p-so.mp4", "name": "Somali", "low": "https://download.ted.com/talks/NickBostrom_2005G-low-so.mp4"}}, "nativeDownloads": {"high": "https://download.ted.com/talks/NickBostrom_2005G-480p.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "medium": "https://download.ted.com/talks/NickBostrom_2005G.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22", "low": "https://download.ted.com/talks/NickBostrom_2005G-light.mp4?apikey=489b859150fc58263f17110eeb44ed5fba4a3b22"}, "id": 44, "audioDownload": null}, "related_talks": [{"viewed_count": 2128581, "hero": "https://pe.tedcdn.com/images/ted/bfd70bdb95ddbee93eb8b8e5d7b1a849cefe2159_1600x1200.jpg", "title": "Is this our final century?", "id": 42, "speaker": "Martin Rees", "duration": 1046, "slug": "martin_rees_asks_is_this_our_final_century"}, {"viewed_count": 1559345, "hero": "https://pe.tedcdn.com/images/ted/7ddac8199e485dc8fe198d34b726f76680e7b86e_2880x1620.jpg", "title": "How technology evolves", "id": 19, "speaker": "Kevin Kelly", "duration": 1200, "slug": "kevin_kelly_on_how_technology_evolves"}, {"viewed_count": 1120440, "hero": "https://pe.tedcdn.com/images/ted/714735a4fcc8eccc6c86beba28fdee415e02628f_2880x1620.jpg", "title": "Progress is not a zero-sum game", "id": 68, "speaker": "Robert Wright", "duration": 1151, "slug": "robert_wright_on_optimism"}, {"viewed_count": 961274, "hero": "https://pe.tedcdn.com/images/ted/bad6d87ce767ae008c56bde103deec058846661e_2880x1620.jpg", "title": "What would happen if we upload our brains to computers?", "id": 2858, "speaker": "Robin Hanson", "duration": 736, "slug": "robin_hanson_what_would_happen_if_we_upload_our_brains_to_computers"}, {"viewed_count": 3298431, "hero": "https://pe.tedcdn.com/images/ted/71994797fda57ea0039fefb37e2f2ebe1adc00c6_2880x1620.jpg", "title": "A roadmap to end aging", "id": 39, "speaker": "Aubrey de Grey", "duration": 1365, "slug": "aubrey_de_grey_says_we_can_avoid_aging"}, {"viewed_count": 498849, "hero": "https://pe.tedcdn.com/images/ted/6fbafd80d79e2de8e034457ad118ea3fba0c6884_2880x1620.jpg", "title": "To upgrade is human", "id": 515, "speaker": "Gregory Stock", "duration": 1071, "slug": "gregory_stock_to_upgrade_is_human"}], "recorded_at": "2005-07-14T00:00:00.000+00:00", "slug": "nick_bostrom_on_our_biggest_problems", "speaker_name": "Nick Bostrom", "viewed_count": 768935, "event_badge": null, "event_blurb": "This talk was presented at an official TED conference, and was featured by our editors on the home page.", "recommendations": null, "corrections": []}], "event": "TEDGlobal 2005", "name": "Nick Bostrom: A philosophical quest for our biggest problems"}'
p368
sS'keywords'
p369
(lp370
Vbiotech
p371
aVculture
p372
aVfuture
p373
aVglobal issues
p374
aVhappiness
p375
aVphilosophy
p376
aVtechnology
p377
asS'datecrawled'
p378
g363
(S'\x07\xe1\n\x17\x03&\x16\r\x88\xba'
tRp379
sS'id'
p380
I44
ss.